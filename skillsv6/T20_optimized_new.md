## TOPIC: T20 – AI Media (Phase 8 Optimized - December 2025)
# Phase 8 MAJOR RESTRUCTURING - Focus on AI Media Generation & Perception
#
# TOPIC CLARIFICATION (Phase 8):
# T20 focuses on AI MEDIA - generation and perception of images, audio, and video/vision
# T21 focuses on CHATBOTS & PROMPTING - text-based AI conversations and prompt engineering
# ChatGPT skills remain in T20 ONLY when directly supporting media workflows (image description, etc.)
#
# CORE SKILL PATHWAYS (Phase 8):
# 1. AI IMAGE GENERATION: DALL-E image creation, prompt refinement, quality evaluation
# 2. AI AUDIO: Text-to-speech synthesis, speech recognition, voice interfaces
# 3. AI VISION/PERCEPTION: Face detection, body tracking, hand detection, gesture recognition
# 4. NEURAL NETWORKS & ML: Building/training models, KNN classification, pattern recognition
# 5. SEMANTIC SEARCH: Vector databases, similarity search, knowledge retrieval
# 6. AI MEDIA ETHICS: Bias detection, accessibility, responsible AI practices
#
# PHASE 8 KEY IMPROVEMENTS:
# 1. SEQUENTIAL SKILL IDS - All skills numbered consecutively within each grade
# 2. CLEARER T20/T21 BOUNDARY - Moved pure chatbot skills to T21, kept media-integration
# 3. EXPANDED K-2 - 8 skills per grade with better granularity for young learners
# 4. STRONGER VERBS - Replaced "understand/identify" with "sort/trace/debug/evaluate"
# 5. BETTER NEURAL NETWORK PROGRESSION - Clear path from concepts to implementation
# 6. ACCESSIBILITY THREAD - Skills for building inclusive AI media applications
#
# SKILL DISTRIBUTION (Phase 8):
# - GK: 8 skills (picture sorting, matching, basic AI media concepts)
# - G1: 8 skills (comparing AI/real media, sequencing, safety)
# - G2: 8 skills (training data analysis, bias awareness, sharing rules)
# - G3: 10 skills (pre-coding: pipelines, templates, evaluation)
# - G4: 12 skills (advanced analysis: complexity, safety, iteration strategies)
# - G5: 16 skills (first coding: DALL-E, TTS, parameter adjustment)
# - G6: 20 skills (face/body detection, speech recognition, moderation)
# - G7: 24 skills (hand detection, neural networks, KNN, multi-modal)
# - G8: 28 skills (deep learning, semantic search, production systems, accessibility)
#
# Total: 134 skills (GK-G8) - focused on AI media generation, perception, and evaluation

Focus: AI-generated images and audio, computer vision (face/body/hand detection), neural networks, semantic search, and AI media evaluation

## GRADE K (8 skills)




ID: T20.GK.01
Topic: T20 – AI Media
Skill: Sort picture cards by real photos vs computer-made images
Description: **Student task:** Sort 12 picture cards into two piles: photos taken by cameras and pictures made by computers. **Visual scenario:** Cards show simple objects (apple, dog, car) with obvious differences—photos have natural backgrounds and lighting, computer images have flat colors and perfect shapes. Cards have visual hints like camera icons or computer icons on the back for self-checking. **Key insight:** Computers can make pictures that look different from real photos.
Activity Type: Unplugged sorting
Estimated Time: 3-4 minutes
CSTA: 1A-CS-01

Dependencies: None




ID: T20.GK.02
Topic: T20 – AI Media
Skill: Match word cards to AI-generated pictures
Description: **Student task:** Match 6 word cards (cat, tree, house, sun, flower, car) to corresponding computer-generated pictures. **Visual scenario:** Word cards have simple text and icons, picture cards show colorful AI-generated images of each word. Students place word card next to matching picture card on a mat. **Key insight:** Computers can make pictures from words we give them.
Activity Type: Unplugged matching
Estimated Time: 2-3 minutes
CSTA: 1A-AP-10

Dependencies: None




ID: T20.GK.03
Topic: T20 – AI Media
Skill: Sort things AI can draw vs things AI cannot draw
Description: **Student task:** Sort 10 cards into two groups: things AI can draw (dog, car, flower, house, rainbow) and things AI cannot draw yet (love, friendship, hunger, being tired, feeling scared). **Visual scenario:** Physical cards show concrete objects on one set and abstract emotion icons on another. Students use sorting mat with "AI Can Draw" and "AI Cannot Draw Yet" headers. **Key insight:** AI can draw things we see but struggles with feelings and ideas.
Activity Type: Unplugged sorting
Estimated Time: 3-4 minutes
CSTA: 1A-IC-16

Dependencies:
* T20.GK.01: Sort picture cards by real photos vs computer-made images




ID: T20.GK.04
Topic: T20 – AI Media
Skill: Match computer voices to different helper devices
Description: **Student task:** Listen to 5 short voice clips and match each to picture cards of devices (phone, tablet, robot toy, smart speaker, talking GPS). **Visual scenario:** Teacher plays audio clips of AI voices saying simple phrases. Students place numbered tokens on picture cards showing which device made which sound. Audio clips demonstrate different voice styles (friendly, robotic, helpful). **Key insight:** Computers can talk using different voices to help us.
Activity Type: Audio matching
Estimated Time: 3-4 minutes
CSTA: 1A-CS-01

Dependencies: None




ID: T20.GK.05
Topic: T20 – AI Media
Skill: Sort activities by human-only vs AI-can-help
Description: **Student task:** Sort 12 activity cards into two piles: things only people can do (hug someone, taste food, feel happy, play with friends) and things AI can help with (draw pictures, make sounds, answer questions, play music). **Visual scenario:** Cards show illustrated scenarios of activities. Students use a sorting mat with two columns and discuss their choices with a partner. **Key insight:** Some things need real people, but AI can help us with other tasks.
Activity Type: Unplugged sorting
Estimated Time: 4-5 minutes
CSTA: 1A-IC-16

Dependencies:
* T20.GK.03: Sort things AI can draw vs things AI cannot draw




ID: T20.GK.06
Topic: T20 – AI Media
Skill: Debug AI picture mistakes by finding wrong details
Description: **Student task:** Find 3 mistakes in each of 4 AI-generated pictures (dog with 5 legs, car with square wheels, person with hand on wrong arm, house with upside-down door). **Visual scenario:** Large picture cards show AI-generated images with obvious errors. Students use red dot stickers to mark mistakes and explain what's wrong. Answer cards show correct versions. **Key insight:** AI sometimes makes mistakes when creating pictures, and people need to check them.
Activity Type: Error detection
Estimated Time: 4-5 minutes
CSTA: 1A-AP-14

Dependencies:
* T20.GK.01: Sort picture cards by real photos vs computer-made images
* T20.GK.02: Match word cards to AI-generated pictures




ID: T20.GK.07
Topic: T20 – AI Media
Skill: Match safe-sharing symbols to AI picture scenarios
Description: **Student task:** Match 6 safe-sharing symbol cards (checkmark for OK, X for not OK) to scenarios showing kids sharing AI pictures (show to teacher=OK, post online alone=not OK, share with parent=OK, send to strangers=not OK). **Visual scenario:** Scenario cards show illustrated situations of children with AI-generated content. Students place green checkmarks or red X cards on each scenario. **Key insight:** We need grown-up permission before sharing AI pictures outside our classroom.
Activity Type: Safety sorting
Estimated Time: 3-4 minutes
CSTA: 1A-IC-18

Dependencies:
* T20.GK.01: Sort picture cards by real photos vs computer-made images




ID: T20.GK.08
Topic: T20 – AI Media
Skill: Predict what picture AI will make from simple word combinations
Description: **Student task:** Look at 2-word combination cards (blue dog, big tree, small car, happy sun) and choose from 3 picture options what the AI would create. **Visual scenario:** Word cards show simple adjective-noun combinations. Students see 3 picture choices and circle the most likely AI output based on the words given. Self-checking answer key on card backs. **Key insight:** AI follows the words we give it to make pictures.
Activity Type: Prediction
Estimated Time: 3-4 minutes
CSTA: 1A-AP-10

Dependencies:
* T20.GK.02: Match word cards to AI-generated pictures


## GRADE 1 (8 skills)




ID: T20.G1.01
Topic: T20 – AI Media
Skill: Compare photo elements vs AI-generated image elements
Description: **Student task:** Examine 6 paired cards (real photo next to AI version of same subject) and find 3 differences in each pair using observation checklist (lighting, shadows, background detail, texture). **Visual scenario:** Large paired cards show side-by-side comparisons (real cat photo vs AI cat, real park vs AI park). Students mark differences on printed checklist with provided categories. **Key insight:** AI images and real photos have different qualities we can learn to notice.
Activity Type: Comparison analysis
Estimated Time: 4-5 minutes
CSTA: 1A-CS-01

Dependencies:
* T20.GK.01: Sort picture cards by real photos vs computer-made images




ID: T20.G1.02
Topic: T20 – AI Media
Skill: Sequence word cards to create specific AI picture instructions
Description: **Student task:** Arrange 4-5 word cards in order to create clear instructions for AI (color + size + object + location). **Visual scenario:** Students have word card sets (red, small, dog, park) and arrange them in sequence boxes numbered 1-4. They compare their sequence to example AI outputs showing what happens with different word orders. Includes 5 different scenarios to practice. **Key insight:** The order and choice of words affects what picture AI creates.
Activity Type: Sequencing
Estimated Time: 4-5 minutes
CSTA: 1A-AP-11

Dependencies:
* T20.GK.02: Match word cards to AI-generated pictures
* T20.GK.08: Predict what picture AI will make from simple word combinations




ID: T20.G1.03
Topic: T20 – AI Media
Skill: Detect AI-generated image errors in realistic scenarios
Description: **Student task:** Examine 5 AI-generated images of everyday scenes and mark errors using error type cards (wrong body parts, impossible physics, mixed-up objects, incorrect counting). **Visual scenario:** Cards show AI images with subtle mistakes (person with extra fingers, reflection that doesn't match, bicycle with uneven wheels). Students place error-type labels on detected problems. **Key insight:** AI can make realistic-looking pictures that have mistakes we need to spot.
Activity Type: Error detection
Estimated Time: 5 minutes
CSTA: 1A-AP-14

Dependencies:
* T20.GK.06: Debug AI picture mistakes by finding wrong details




ID: T20.G1.04
Topic: T20 – AI Media
Skill: Explain why detected AI image errors happened
Description: **Student task:** For 4 AI images with marked errors, match explanation cards describing why the error occurred (AI doesn't understand hands, AI mixed training images, AI doesn't know real physics). **Visual scenario:** Building on detected errors from previous skill, students now select from explanation cards showing simple reasons for each type of mistake. Uses same error images but focuses on the "why" rather than detection. **Key insight:** AI makes predictable types of mistakes because of how it learns from pictures.
Activity Type: Explanation matching
Estimated Time: 4-5 minutes
CSTA: 1A-AP-14

Dependencies:
* T20.G1.03: Detect AI-generated image errors in realistic scenarios




ID: T20.G1.05
Topic: T20 – AI Media
Skill: Distinguish human voices from AI-generated voices
Description: **Student task:** Listen to 10 short audio clips (5 human, 5 AI) and sort numbered tokens into "Human Voice" or "AI Voice" categories based on voice qualities. **Visual scenario:** Teacher plays audio clips of people and AI saying the same sentences. Students listen for clues (breathing sounds, natural pauses, emotion variation, slight imperfections) and place tokens on sorting mat. Includes discussion guide for checking answers. **Key insight:** AI voices sound almost real but have small differences we can learn to hear.
Activity Type: Audio sorting
Estimated Time: 5 minutes
CSTA: 1A-CS-01

Dependencies:
* T20.GK.04: Match computer voices to different helper devices




ID: T20.G1.06
Topic: T20 – AI Media
Skill: Match emotion words to tasks only humans can do
Description: **Student task:** Connect 8 emotion/creativity cards (feeling proud, caring for someone, being creative, making a friend laugh) to 8 task cards showing why humans are needed (art needs feeling, helping needs caring, jokes need understanding). **Visual scenario:** Students draw lines or place strings connecting emotion cards to specific task examples. Each task card shows an illustrated scenario requiring human qualities. Includes self-checking answer guide. **Key insight:** Creative and emotional tasks need real human feelings that AI doesn't have.
Activity Type: Matching
Estimated Time: 4-5 minutes
CSTA: 1A-IC-16

Dependencies:
* T20.GK.03: Sort things AI can draw vs things AI cannot draw
* T20.GK.05: Sort activities by human-only vs AI-can-help




ID: T20.G1.07
Topic: T20 – AI Media
Skill: Find private information in AI-generated content before sharing
Description: **Student task:** Review 6 AI-generated images showing classroom scenarios and mark with red dots any private information visible (names, faces, addresses, school names). **Visual scenario:** Picture cards show AI images that students might create (class photo, birthday invitation, house drawing). Students use red sticker dots to cover private details and green checkmarks for safe content. Includes privacy checklist card. **Key insight:** Even AI-generated pictures can show private information we shouldn't share.
Activity Type: Privacy check
Estimated Time: 5 minutes
CSTA: 1A-IC-18

Dependencies:
* T20.GK.07: Match safe-sharing symbols to AI picture scenarios




ID: T20.G1.08
Topic: T20 – AI Media
Skill: Sort sharing scenarios by permission level needed
Description: **Student task:** Sort 10 scenario cards into 3 permission categories (no permission needed, ask teacher, ask parent/guardian) based on what's being shared and where. **Visual scenario:** Cards show specific situations (showing AI picture to classmate, posting AI video online, printing AI image for home, emailing AI creation to grandparent). Students place cards in 3 zones on sorting mat and explain reasoning. **Key insight:** Different ways of sharing AI content need different levels of permission from adults.
Activity Type: Permission sorting
Estimated Time: 5 minutes
CSTA: 1A-IC-18

Dependencies:
* T20.G1.07: Find private information in AI-generated content before sharing
* T20.GK.07: Match safe-sharing symbols to AI picture scenarios


## GRADE 2 (8 skills)




ID: T20.G2.01
Topic: T20 – AI Media
Skill: Analyze training data to predict AI image output
Description: **Student task:** Examine 8 training image cards showing what AI learned from (all dogs are golden retrievers, all cars are red, all trees have leaves) and predict what the AI will create when asked for "dog", "car", or "tree". **Visual scenario:** Students see sets of training images grouped by category, then choose from 3 possible output predictions. Includes explanation cards showing how limited training creates limited outputs. 4 different scenarios to analyze. **Key insight:** AI creates images based on patterns in pictures it learned from, so training data affects outputs.
Activity Type: Prediction analysis
Estimated Time: 5 minutes
CSTA: 1B-AP-10

Dependencies:
* T20.G1.01: Compare photo elements vs AI-generated image elements
* T20.G1.02: Sequence word cards to create specific AI picture instructions




ID: T20.G2.02
Topic: T20 – AI Media
Skill: Create detailed word sequences for complex AI images
Description: **Student task:** Build 6-8 word instruction sequences using provided word cards (adjectives, nouns, actions, locations, styles) to create specific complex images. **Visual scenario:** Students have expanded word card sets organized by category (colors, sizes, objects, places, art styles). They arrange cards in sequence frames to build detailed prompts like "large blue robot dancing in city park cartoon style". Includes 4 target images to recreate through word sequencing. **Key insight:** More detailed and specific word instructions help AI create images closer to what we want.
Activity Type: Prompt building
Estimated Time: 5 minutes
CSTA: 1B-AP-11

Dependencies:
* T20.G1.02: Sequence word cards to create specific AI picture instructions




ID: T20.G2.03
Topic: T20 – AI Media
Skill: Debug word sequences when AI output doesn't match intent
Description: **Student task:** Given 5 word sequences and their unexpected AI outputs, identify which words caused the problem and suggest replacement words. **Visual scenario:** Cards show original word sequence, the AI's output image, and the intended output image. Students circle problematic words and select better word choices from provided options. Includes common issues like ambiguous words, conflicting descriptions, or missing key details. **Key insight:** When AI creates wrong images, we can fix our word instructions to get better results.
Activity Type: Debugging
Estimated Time: 5 minutes
CSTA: 1B-AP-14

Dependencies:
* T20.G2.02: Create detailed word sequences for complex AI images




ID: T20.G2.04
Topic: T20 – AI Media
Skill: Compare AI voice qualities across different applications
Description: **Student task:** Listen to 6 different AI voices (GPS navigation, audiobook reader, voice assistant, character voice, translation voice, accessibility reader) and match each to its best use case card. **Visual scenario:** Teacher plays audio samples of different AI voice types. Students have application cards showing when each voice type works best and match voices to uses. Includes characteristic cards (clear/fast, expressive/slow, friendly/helpful) to describe each voice. **Key insight:** Different AI voices are designed for different purposes and have different strengths.
Activity Type: Audio analysis
Estimated Time: 5 minutes
CSTA: 1B-CS-01

Dependencies:
* T20.G1.05: Distinguish human voices from AI-generated voices




ID: T20.G2.05
Topic: T20 – AI Media
Skill: Sort tasks requiring human creativity vs AI assistance
Description: **Student task:** Sort 12 creative task cards into 3 categories: needs human creativity, AI can assist humans, AI can do independently. **Visual scenario:** Task cards show activities (write a poem about friendship, resize a photo, choose a birthday gift for friend, remove photo background, paint feelings, generate stock image). Students place cards in 3-column sorting mat and justify choices using reasoning cards. **Key insight:** Some creative tasks need human feelings and choices, while AI can help with technical parts.
Activity Type: Creativity sorting
Estimated Time: 5 minutes
CSTA: 1B-IC-18

Dependencies:
* T20.G1.06: Match emotion words to tasks only humans can do




ID: T20.G2.06
Topic: T20 – AI Media
Skill: Predict consequences of sharing AI content in different contexts
Description: **Student task:** Given 8 sharing scenario cards, predict and match consequence cards (positive, neutral, negative outcomes) explaining what might happen. **Visual scenario:** Scenario cards show situations like posting AI art claiming it's hand-drawn, sharing AI images without labeling them as AI-made, using AI voice for school project without permission. Students match to outcome cards and discuss reasoning. **Key insight:** How we share and label AI content affects trust and follows rules about honesty.
Activity Type: Consequence prediction
Estimated Time: 5 minutes
CSTA: 1B-IC-18

Dependencies:
* T20.G1.08: Sort sharing scenarios by permission level needed




ID: T20.G2.07
Topic: T20 – AI Media
Skill: Evaluate AI-generated images for bias in representation
Description: **Student task:** Examine 6 sets of AI-generated images for common subjects (doctors, teachers, engineers, nurses, chefs, scientists) and identify if all images show diversity or show bias patterns. **Visual scenario:** Each set has 4 AI images for same profession. Students use diversity checklist cards (gender, age, race, ability) to mark whether images show variety or patterns. Includes comparison cards showing balanced vs biased sets. **Key insight:** AI sometimes creates biased images based on biased training data, and we should notice these patterns.
Activity Type: Bias detection
Estimated Time: 5 minutes
CSTA: 1B-IC-20

Dependencies:
* T20.G2.01: Analyze training data to predict AI image output
* T20.G1.01: Compare photo elements vs AI-generated image elements




ID: T20.G2.08
Topic: T20 – AI Media
Skill: Create sharing permission checklist for AI media projects
Description: **Student task:** Build a decision flowchart using 10 question cards to determine if AI content is ready to share (Is it labeled as AI? Does it show private info? Do you have permission? Is it appropriate? Is it honest?). **Visual scenario:** Students arrange question cards in logical order on a flowchart mat with yes/no paths leading to "OK to share," "Ask adult first," or "Don't share" endpoints. Test flowchart with 5 example scenarios. **Key insight:** Following a checklist helps us make safe and responsible decisions about sharing AI content.
Activity Type: Checklist creation
Estimated Time: 5 minutes
CSTA: 1B-IC-18

Dependencies:
* T20.G2.06: Predict consequences of sharing AI content in different contexts
* T20.G1.07: Find private information in AI-generated content before sharing


## GRADE 3 (10 skills)




ID: T20.G3.01
Topic: T20 – AI Media
Skill: Distinguish AI-generated images from human-created images using visual clues
Description: Students examine pairs of images (one AI-generated, one human-created) and identify which is which based on observable characteristics. They list specific visual clues such as distorted hands, unusual textures, impossible reflections, or unnatural lighting. Students categorize these clues into categories like "anatomy errors," "lighting problems," and "background inconsistencies." This foundational skill builds critical evaluation abilities needed for analyzing AI media outputs.
Activity Type: Analysis
Estimated Time: 15-20 minutes
CSTA: 1B-IC-18

Dependencies: None




ID: T20.G3.02
Topic: T20 – AI Media
Skill: Label components of an AI image generation pipeline
Description: Students receive a visual diagram showing the basic pipeline of text-to-image AI systems with unlabeled boxes and arrows. They label each component: "text prompt input," "AI processing," "image output," and "feedback/revision." Students match simple explanations to each stage, such as "where you type what you want" or "where the AI creates the picture." They trace the flow from start to finish using arrows to show how information moves through the system.
Activity Type: Diagram
Estimated Time: 10-15 minutes
CSTA: 1B-AP-08

Dependencies: None




ID: T20.G3.03
Topic: T20 – AI Media
Skill: Complete template-based prompts for AI image generation
Description: Students use pre-designed prompt templates with fill-in-the-blank sections to create complete image generation prompts. Templates provide structure like "[subject] in [location] during [time of day] with [mood/style]." Students select appropriate words from provided word banks to fill each blank, ensuring the completed prompt makes logical sense. They compare how different word choices create different mental images and predict what the AI might generate from their completed prompts.
Activity Type: Template completion
Estimated Time: 15-20 minutes
CSTA: 1B-AP-11

Dependencies:
* T20.G3.02: Label components of an AI image generation pipeline




ID: T20.G3.04
Topic: T20 – AI Media
Skill: Predict which text-to-image prompts will produce better results
Description: Students examine pairs of prompts for the same general idea (e.g., "a dog" vs. "a golden retriever puppy playing in a sunny park") and predict which will produce a better AI image. They justify their predictions by identifying specific, descriptive words versus vague, general words. Students create a checklist of "good prompt qualities" including specificity, clear descriptions, and concrete details. They apply this checklist to evaluate and rank additional prompts from best to worst.
Activity Type: Prediction
Estimated Time: 15-20 minutes
CSTA: 1B-AP-14

Dependencies:
* T20.G3.03: Complete template-based prompts for AI image generation




ID: T20.G3.05
Topic: T20 – AI Media
Skill: Categorize AI media types by input and output
Description: Students sort different AI media tools into categories based on what they take as input and what they produce as output. Categories include text-to-image, text-to-audio, image-to-text, and audio-to-text. They receive cards describing various AI tools and place them in a grid organized by input type (columns) and output type (rows). Students identify patterns such as "AI can transform between different media types" and discuss real-world applications for each category.
Activity Type: Categorization
Estimated Time: 15-20 minutes
CSTA: 1B-IC-18

Dependencies:
* T20.G3.02: Label components of an AI image generation pipeline




ID: T20.G3.06
Topic: T20 – AI Media
Skill: Flag safe versus unsafe information in AI media prompts
Description: Students review example prompts for AI image and audio generation and flag those containing private information, inappropriate content, or personally identifiable details. They use a simple safety checklist covering categories like "full names," "addresses," "private photos," and "inappropriate requests." Students rewrite flagged prompts to remove unsafe elements while maintaining the creative intent. They discuss why protecting privacy matters when using AI tools and create a classroom "safe prompting" poster.
Activity Type: Safety analysis
Estimated Time: 20-25 minutes
CSTA: 1B-IC-21

Dependencies:
* T20.G3.03: Complete template-based prompts for AI image generation




ID: T20.G3.07
Topic: T20 – AI Media
Skill: Trace the text-to-speech generation pipeline step-by-step
Description: Students follow a simplified diagram of text-to-speech AI systems and trace each step from text input to audio output. They label stages including "text analysis," "pronunciation selection," "voice synthesis," and "audio output." For each stage, students describe in simple terms what happens (e.g., "the AI breaks words into sounds" or "the AI chooses how to say each word"). They identify where human choices matter, such as selecting voice characteristics or adjusting speed.
Activity Type: Diagram tracing
Estimated Time: 15-20 minutes
CSTA: 1B-AP-08

Dependencies:
* T20.G3.02: Label components of an AI image generation pipeline




ID: T20.G3.08
Topic: T20 – AI Media
Skill: Evaluate AI-generated images for quality and accuracy
Description: Students assess AI-generated images using a simple rubric with criteria such as "matches the prompt," "realistic details," "no obvious errors," and "appropriate composition." They rate each image on a 1-3 scale for each criterion and calculate a total quality score. Students identify specific strengths (e.g., "beautiful colors," "correct number of objects") and weaknesses (e.g., "distorted fingers," "blurry background"). They discuss how evaluation helps users decide whether to accept, revise, or regenerate AI outputs.
Activity Type: Rubric evaluation
Estimated Time: 20-25 minutes
CSTA: 1B-IC-18

Dependencies:
* T20.G3.01: Distinguish AI-generated images from human-created images using visual clues
* T20.G3.04: Predict which text-to-image prompts will produce better results




ID: T20.G3.09
Topic: T20 – AI Media
Skill: Match prompt modifications to expected changes in AI images
Description: Students receive a base prompt and several modified versions with one change each (e.g., changing time of day, adding weather, switching location). They predict how each modification will change the resulting AI image by drawing or describing the expected difference. Students match "before and after" image pairs showing actual AI outputs to the corresponding prompt modifications. They identify which types of prompt changes create dramatic visual differences versus subtle adjustments.
Activity Type: Prediction matching
Estimated Time: 20-25 minutes
CSTA: 1B-AP-14

Dependencies:
* T20.G3.04: Predict which text-to-image prompts will produce better results
* T20.G3.08: Evaluate AI-generated images for quality and accuracy




ID: T20.G3.10
Topic: T20 – AI Media
Skill: Construct multi-stage improvement plans for failed AI image outputs
Description: Students analyze AI-generated images that failed to match their prompts and create step-by-step improvement plans. They identify the specific problems (e.g., "missing object," "wrong color," "incorrect setting"), prioritize which to fix first, and write revised prompts addressing each issue. Students organize their plans using numbered steps showing the progression from original prompt to first revision to final revision. They explain why each revision should improve the output and predict the cumulative effect of all changes.
Activity Type: Revision planning
Estimated Time: 25-30 minutes
CSTA: 1B-AP-11

Dependencies:
* T20.G3.08: Evaluate AI-generated images for quality and accuracy
* T20.G3.09: Match prompt modifications to expected changes in AI images


## GRADE 4 (12 skills)




ID: T20.G4.01
Topic: T20 – AI Media
Skill: Analyze how prompt specificity affects AI image generation accuracy
Description: Students conduct structured comparisons of AI outputs from prompts with varying levels of specificity. They create a spectrum from "vague" to "highly specific" and place example prompts along this spectrum, then examine corresponding AI outputs. Students quantify specificity by counting descriptive adjectives, concrete nouns, and contextual details in each prompt. They graph the relationship between specificity score and output quality score, identifying the threshold where additional details stop improving results. Students document patterns such as "more details usually helps, but too many conflicting details confuses the AI."
Activity Type: Comparative analysis
Estimated Time: 25-30 minutes
CSTA: 2-IC-20

Dependencies:
* T20.G3.08: Evaluate AI-generated images for quality and accuracy
* T20.G3.04: Predict which text-to-image prompts will produce better results




ID: T20.G4.02
Topic: T20 – AI Media
Skill: Construct detailed pipeline diagrams for multi-modal AI systems
Description: Students build comprehensive pipeline diagrams for AI systems that process multiple media types (e.g., image-to-text-to-speech or text-to-image-to-description). They identify and label all components including input processing, AI transformation stages, intermediate outputs, and final outputs. Students use different shapes and colors to distinguish between different media types flowing through the pipeline and add decision points where the AI makes choices. They annotate each connection with the type of data being transferred and identify potential failure points in the pipeline.
Activity Type: Diagram construction
Estimated Time: 30-35 minutes
CSTA: 2-AP-10

Dependencies:
* T20.G3.02: Label components of an AI image generation pipeline
* T20.G3.07: Trace the text-to-speech generation pipeline step-by-step




ID: T20.G4.03
Topic: T20 – AI Media
Skill: Debug prompts by tracing AI misinterpretations
Description: Students analyze failed AI outputs and trace backward to identify which parts of the prompt the AI misinterpreted. They create two-column tables showing "what I meant" versus "what the AI understood" for each prompt component. Students categorize common misinterpretation types such as ambiguous words, conflicting instructions, unclear spatial relationships, or assumed context the AI lacks. They develop "debugging questions" to ask about any prompt: "Could this word mean two different things?" or "Did I specify where objects should be positioned?"
Activity Type: Debugging analysis
Estimated Time: 25-30 minutes
CSTA: 2-AP-17

Dependencies:
* T20.G3.10: Construct multi-stage improvement plans for failed AI image outputs
* T20.G4.01: Analyze how prompt specificity affects AI image generation accuracy




ID: T20.G4.04
Topic: T20 – AI Media
Skill: Evaluate AI image generation biases and limitations
Description: Students examine collections of AI-generated images for the same prompt and identify patterns in how the AI represents people, places, and objects. They document biases such as "default assumptions" (e.g., always shows doctors as one gender, always shows certain professions with specific characteristics). Students categorize limitations like "things AI can't do well" (text in images, hands, complex spatial arrangements) and "things AI does inconsistently" (maintaining same character across images). They create visual charts showing which categories of prompts produce reliable versus unreliable results.
Activity Type: Bias evaluation
Estimated Time: 30-35 minutes
CSTA: 2-IC-20

Dependencies:
* T20.G3.08: Evaluate AI-generated images for quality and accuracy
* T20.G4.01: Analyze how prompt specificity affects AI image generation accuracy




ID: T20.G4.05
Topic: T20 – AI Media
Skill: Construct structured prompt templates with conditional options
Description: Students design advanced prompt templates that include conditional branches (e.g., "If outdoor scene, specify weather and time of day; if indoor scene, specify lighting and room type"). They create template flowcharts showing decision points where users choose different paths leading to different required fields. Students test their templates by having peers use them to generate prompts, collecting feedback on clarity and completeness. They refine templates based on which paths users find confusing and which produce consistently good AI outputs.
Activity Type: Template design
Estimated Time: 30-35 minutes
CSTA: 2-AP-13

Dependencies:
* T20.G3.03: Complete template-based prompts for AI image generation
* T20.G4.01: Analyze how prompt specificity affects AI image generation accuracy




ID: T20.G4.06
Topic: T20 – AI Media
Skill: Trace information flow in image-to-text AI systems
Description: Students map the complete pipeline of AI vision systems that generate text descriptions from images. They identify stages including image input, feature detection, object recognition, relationship analysis, and text generation. For each stage, students describe what information is extracted (e.g., "identifies shapes and colors" then "recognizes objects" then "determines spatial relationships"). They annotate diagrams with examples of intermediate data at each stage and identify where errors might compound as incorrect information flows to later stages.
Activity Type: Pipeline tracing
Estimated Time: 25-30 minutes
CSTA: 2-AP-10

Dependencies:
* T20.G3.02: Label components of an AI image generation pipeline
* T20.G4.02: Construct detailed pipeline diagrams for multi-modal AI systems




ID: T20.G4.07
Topic: T20 – AI Media
Skill: Predict AI failures based on prompt complexity analysis
Description: Students analyze prompts to predict specific ways the AI might fail before seeing any output. They use a complexity scoring system evaluating factors like number of objects, spatial relationships specified, abstract concepts included, and potential contradictions. Students map these complexity factors to known AI weaknesses and predict likely failure modes: "Will probably distort hands," "May confuse foreground and background," "Might ignore the smallest objects." They test predictions against actual AI outputs and refine their prediction criteria based on results.
Activity Type: Failure prediction
Estimated Time: 30-35 minutes
CSTA: 2-AP-17

Dependencies:
* T20.G4.03: Debug prompts by tracing AI misinterpretations
* T20.G4.04: Evaluate AI image generation biases and limitations




ID: T20.G4.08
Topic: T20 – AI Media
Skill: Analyze safety and privacy risks in AI media generation workflows
Description: Students examine complete workflows from prompt creation through output sharing and identify safety/privacy risks at each stage. They create risk assessment matrices categorizing risks by severity (low/medium/high) and likelihood, covering issues like unintentional inclusion of personal information, generating inappropriate content, copyright concerns with training data, and misuse of generated media. Students develop stage-specific safety checklists and design workflow modifications that reduce identified risks. They compare different AI media types (image, audio, video) for unique safety considerations.
Activity Type: Risk analysis
Estimated Time: 30-35 minutes
CSTA: 2-IC-23

Dependencies:
* T20.G3.06: Flag safe versus unsafe information in AI media prompts
* T20.G4.02: Construct detailed pipeline diagrams for multi-modal AI systems




ID: T20.G4.09
Topic: T20 – AI Media
Skill: Construct revision strategies for iterative prompt improvement
Description: Students develop systematic strategies for iteratively improving prompts based on AI output analysis. They create decision trees showing "If output has [specific problem], then modify prompt by [specific change]" for common issues. Students design revision tracking templates that document original prompt, identified problems, modifications made, and results for each iteration. They analyze multi-iteration examples to identify efficient versus inefficient revision strategies, noting when users make redundant changes or fail to address root causes. Students establish "stopping criteria" for when a prompt is good enough versus when to try a different approach.
Activity Type: Strategy development
Estimated Time: 35-40 minutes
CSTA: 2-AP-17

Dependencies:
* T20.G3.10: Construct multi-stage improvement plans for failed AI image outputs
* T20.G4.03: Debug prompts by tracing AI misinterpretations




ID: T20.G4.10
Topic: T20 – AI Media
Skill: Evaluate trade-offs between prompt detail and AI creativity
Description: Students investigate the balance between providing detailed constraints and allowing AI creative freedom. They generate AI images using matched pairs of prompts (highly constrained vs. open-ended) for the same general concept and evaluate results on both "matches intent" and "creative/interesting." Students create scatter plots positioning outputs on axes of control versus creativity and identify the "sweet spot" for different use cases. They articulate when strict control matters (e.g., matching specific requirements) versus when creative freedom is valuable (e.g., exploring ideas, inspiration).
Activity Type: Trade-off analysis
Estimated Time: 30-35 minutes
CSTA: 2-IC-20

Dependencies:
* T20.G4.01: Analyze how prompt specificity affects AI image generation accuracy
* T20.G4.04: Evaluate AI image generation biases and limitations




ID: T20.G4.11
Topic: T20 – AI Media
Skill: Trace multi-step AI media transformation chains
Description: Students diagram and analyze complex AI workflows involving multiple transformation steps (e.g., text-to-image, then image-to-description, then description-to-speech). They trace how information changes at each step, identifying what is preserved, what is lost, and what is added. Students predict how errors or biases compound through transformation chains by analyzing where each stage might introduce distortion. They compare the final output to the original input and quantify information degradation, creating visual "quality degradation graphs" showing how fidelity decreases with each transformation.
Activity Type: Chain analysis
Estimated Time: 35-40 minutes
CSTA: 2-AP-10

Dependencies:
* T20.G4.02: Construct detailed pipeline diagrams for multi-modal AI systems
* T20.G4.06: Trace information flow in image-to-text AI systems




ID: T20.G4.12
Topic: T20 – AI Media
Skill: Construct comprehensive evaluation frameworks for AI media outputs
Description: Students design multi-criteria evaluation frameworks for assessing AI-generated media across technical quality, prompt alignment, creativity, appropriateness, and usability dimensions. They define specific measurable indicators for each criterion (e.g., for "technical quality": resolution, artifact count, consistency) and create scoring rubrics with detailed descriptors for each level. Students test their frameworks by having multiple evaluators rate the same AI outputs and calculating inter-rater reliability. They refine criteria and rubrics based on which dimensions show inconsistent scoring and add examples to clarify ambiguous descriptors.
Activity Type: Framework construction
Estimated Time: 40-45 minutes
CSTA: 2-AP-17

Dependencies:
* T20.G3.08: Evaluate AI-generated images for quality and accuracy
* T20.G4.04: Evaluate AI image generation biases and limitations
* T20.G4.10: Evaluate trade-offs between prompt detail and AI creativity


## GRADE 5 (16 skills) - First AI Media Coding




ID: T20.G5.01
Topic: T20 – AI Media
Skill: Generate an AI image with DALL-E using a descriptive prompt
Description: Students use the `OpenAI DALL-E: generate image with request [DESCRIPTION] resolution [RESOLUTION v]` block to create their first AI-generated image. They write a descriptive prompt (e.g., "a friendly robot in a colorful garden") and select a resolution (256x256, 512x512, or 1024x1024). The reporter block returns an image URL that can be loaded into their project. Students observe how the AI interprets text descriptions to create visual media.
CSTA: 2-AP-16

Dependencies:
* T20.G4.10: Evaluate trade-offs between prompt detail and AI creativity
* T06.G3.02: Build a green-flag script that runs a 3-5 block sequence
* T09.G3.01: Create a new variable with a descriptive name




ID: T20.G5.02
Topic: T20 – AI Media
Skill: Search the AI image library for pre-made assets
Description: Students use the `search for AI image of [TYPE v] with query [QUERY]` block to find pre-generated images from CreatiCode's curated AI library. TYPE options include Object, Character, and Backdrop. They compare library search (fast, safe, curated) versus custom DALL-E generation (specific, original) to understand when each approach is appropriate. Students learn to select the right tool for their project needs.
CSTA: 2-IC-20

Dependencies:
* T20.G5.01: Generate an AI image with DALL-E using a descriptive prompt
* T06.G3.02: Build a green-flag script that runs a 3-5 block sequence




ID: T20.G5.03
Topic: T20 – AI Media
Skill: Refine image prompts based on generation results
Description: Students practice iterative prompt refinement: generate an image, evaluate what's missing or wrong, add descriptive details, regenerate. They document 3 iterations showing how adding specific details (colors, actions, settings, mood) improves results. For example: "cat" → "orange tabby cat" → "orange tabby cat sitting on a windowsill at sunset." This teaches the generate-evaluate-refine cycle essential for AI media creation.
CSTA: 2-AP-17

Dependencies:
* T20.G5.01: Generate an AI image with DALL-E using a descriptive prompt
* T20.G4.09: Construct revision strategies for iterative prompt improvement




ID: T20.G5.04
Topic: T20 – AI Media
Skill: Adjust image resolution based on project requirements
Description: Students experiment with DALL-E's three resolution options: 256x256 (fast, small file, good for testing), 512x512 (balanced quality and speed), 1024x1024 (high quality, slower, larger file). They generate the same image at different resolutions and compare generation time, file size, and visual quality. Students learn to choose appropriate resolution based on project context (testing vs final product, mobile vs desktop).
CSTA: 2-IC-20

Dependencies:
* T20.G5.01: Generate an AI image with DALL-E using a descriptive prompt
* T06.G3.02: Build a green-flag script that runs a 3-5 block sequence




ID: T20.G5.05
Topic: T20 – AI Media
Skill: Implement basic text-to-speech with default parameters
Description: Students use the `say [TEXT] in [LANGUAGE v] as [VOICETYPE v] speed (SPEEDRATIO) pitch (PITCHRATIO) volume (VOLUMERATIO) store sound as [SOUNDNAME]` block to convert text into spoken audio. They start with default values (speed 1.0, pitch 1.0, volume 1.0) and basic voice types (Male, Female). Students observe how text input transforms into audio output, connecting text data to media generation.
CSTA: 2-AP-16

Dependencies:
* T06.G3.02: Build a green-flag script that runs a 3-5 block sequence
* T20.G3.01: Distinguish AI-generated images from human-created images using visual clues
* T09.G3.01: Create a new variable with a descriptive name




ID: T20.G5.06
Topic: T20 – AI Media
Skill: Select appropriate voice types for different characters
Description: Students explore available voice types: Male, Female, Boy, Girl, Male2, Female2, Male3, Female3. They select voices that match character personalities in stories or games. For example, use "Boy" voice for a young hero, "Female2" for a narrator. Students experiment with multiple voices in one project to create distinct character voices, learning how voice selection affects audience perception.
CSTA: 2-IC-20

Dependencies:
* T20.G5.05: Implement basic text-to-speech with default parameters
* T06.G3.02: Build a green-flag script that runs a 3-5 block sequence




ID: T20.G5.07
Topic: T20 – AI Media
Skill: Adjust speech speed for clarity and effect
Description: Students experiment with the speed parameter (0.5-2.0): slow speech (0.5-0.8) for emphasis or instructions, normal speed (1.0) for dialogue, fast speech (1.2-2.0) for excitement or comedy. They generate the same sentence at three different speeds and observe how speed affects comprehension and emotional tone. Students learn to match speech speed to content purpose.
CSTA: 2-AP-16

Dependencies:
* T20.G5.05: Implement basic text-to-speech with default parameters
* T06.G3.02: Build a green-flag script that runs a 3-5 block sequence




ID: T20.G5.08
Topic: T20 – AI Media
Skill: Adjust speech pitch to create character voices
Description: Students experiment with the pitch parameter (0.5-2.0): low pitch (0.5-0.8) for deep/serious characters, normal pitch (1.0) for neutral voices, high pitch (1.2-2.0) for young/excited characters. They create 3-4 distinct character voices by combining different voice types and pitch values. This teaches how parameter combinations create variety in AI-generated audio.
CSTA: 2-AP-16

Dependencies:
* T20.G5.05: Implement basic text-to-speech with default parameters
* T06.G3.02: Build a green-flag script that runs a 3-5 block sequence




ID: T20.G5.09
Topic: T20 – AI Media
Skill: Control speech volume for different audio contexts
Description: Students use the volume parameter (0.5-2.0) to control speech loudness: quiet (0.5-0.7) for background narration, normal (1.0) for dialogue, loud (1.3-2.0) for announcements or emphasis. They build a project with multiple audio elements at different volumes to create audio depth. Students learn volume mixing principles for multi-voice projects.
CSTA: 2-AP-16

Dependencies:
* T20.G5.05: Implement basic text-to-speech with default parameters
* T06.G3.02: Build a green-flag script that runs a 3-5 block sequence




ID: T20.G5.10
Topic: T20 – AI Media
Skill: Stop speech playback with the stop speaking block
Description: Students use the `stop speaking` block to interrupt ongoing text-to-speech playback. They implement user controls (button clicks, key presses) to stop speech mid-sentence. Students build interactive projects where users can skip long narration or stop dialogue when needed. This teaches responsive audio control in AI media applications.
CSTA: 2-AP-16

Dependencies:
* T20.G5.05: Implement basic text-to-speech with default parameters
* T06.G3.02: Build a green-flag script that runs a 3-5 block sequence
* T06.G3.06: Trigger code when specific keys are pressed




ID: T20.G5.11
Topic: T20 – AI Media
Skill: Select language for multilingual text-to-speech
Description: Students explore language options in text-to-speech (30+ languages including English, Spanish, French, Chinese, Japanese, German, Hindi). They create multilingual projects where characters speak different languages, or build language learning tools. Students observe how the same text sounds different in various languages and learn to match language selection to audience needs.
CSTA: 2-IC-20

Dependencies:
* T20.G5.05: Implement basic text-to-speech with default parameters
* T06.G3.02: Build a green-flag script that runs a 3-5 block sequence




ID: T20.G5.12
Topic: T20 – AI Media
Skill: Store generated speech as named sound variables
Description: Students use the `store sound as [SOUNDNAME]` parameter to save text-to-speech output with descriptive names. They generate multiple speech segments stored as different variables (e.g., "intro_narration," "character1_dialogue," "instructions"). Students replay stored sounds using sound playback blocks, learning to manage multiple audio assets in projects. This prepares for speech reuse and audio library management.
CSTA: 2-AP-16

Dependencies:
* T20.G5.05: Implement basic text-to-speech with default parameters
* T06.G3.02: Build a green-flag script that runs a 3-5 block sequence
* T09.G3.01: Create a new variable with a descriptive name




ID: T20.G5.13
Topic: T20 – AI Media
Skill: Debug AI image generation failures
Description: Students troubleshoot common DALL-E issues: blank results (prompt contains blocked content—revise to remove sensitive terms), slow generation (high resolution selected—try lower resolution for testing), image doesn't match expectation (prompt too vague—add specific details). They use systematic debugging: check prompt text, verify resolution setting, test with simpler prompts, add error handling to check if image URL is valid before using it.
CSTA: 2-AP-17

Dependencies:
* T20.G5.01: Generate an AI image with DALL-E using a descriptive prompt
* T08.G4.14: Add else to handle the opposite case
* T06.G3.02: Build a green-flag script that runs a 3-5 block sequence




ID: T20.G5.14
Topic: T20 – AI Media
Skill: Debug text-to-speech parameter issues
Description: Students identify and fix common TTS problems: no sound (volume set too low or device muted—check volume parameter and system settings), speech too fast/slow to understand (speed parameter extreme—adjust to 0.8-1.5 range), robotic sound (text contains special characters—remove symbols, use plain text), wrong language (language parameter doesn't match text—verify language selection). They build debugging checklists for audio issues.
CSTA: 2-AP-17

Dependencies:
* T20.G5.05: Implement basic text-to-speech with default parameters
* T08.G4.14: Add else to handle the opposite case
* T06.G3.02: Build a green-flag script that runs a 3-5 block sequence




ID: T20.G5.15
Topic: T20 – AI Media
Skill: Combine image generation and text-to-speech in one project
Description: Students build a project using both DALL-E image generation and text-to-speech together. For example: generate a scene image based on user input, then narrate what's happening in the scene using TTS. They coordinate timing between image loading and speech playback, learning to orchestrate multiple AI media outputs. This integrates image and audio AI capabilities in cohesive projects.
CSTA: 2-AP-13

Dependencies:
* T20.G5.01: Generate an AI image with DALL-E using a descriptive prompt
* T20.G5.05: Implement basic text-to-speech with default parameters
* T06.G3.02: Build a green-flag script that runs a 3-5 block sequence
* T09.G3.01: Create a new variable with a descriptive name




ID: T20.G5.16
Topic: T20 – AI Media
Skill: Use ChatGPT to generate image descriptions for DALL-E
Description: Students use the `OpenAI ChatGPT: request [PROMPT] result [VARIABLE v] mode [MODE v] length [MAXLENGTH] temperature [TEMPERATURE] session [SESSIONTYPE v]` block to ask ChatGPT for detailed image descriptions, then use those descriptions in DALL-E prompts. For example, ask "Describe a magical forest scene in detail" and use the response in DALL-E. This demonstrates AI-to-AI workflows where text generation enhances media generation.
CSTA: 2-AP-13

Dependencies:
* T20.G5.01: Generate an AI image with DALL-E using a descriptive prompt
* T06.G3.02: Build a green-flag script that runs a 3-5 block sequence
* T09.G3.01: Create a new variable with a descriptive name


## GRADE 6 (20 skills) - Face/Body Detection, Speech Recognition, Moderation




ID: T20.G6.01
Topic: T20 – AI Media
Skill: Implement Azure speech recognition with start and end blocks
Description: Students use `start recognizing speech in [LANGUAGE v] record as [SOUNDNAME]` to begin recording audio and `end speech recognition` to stop recording. They build projects where users speak commands or answers, and the system transcribes speech to text. Students access transcribed text using the `text from speech` reporter block. This introduces real-time speech-to-text capabilities for interactive applications.
CSTA: 2-AP-16

Dependencies:
* T20.G5.05: Implement basic text-to-speech with default parameters
* T06.G3.06: Trigger code when specific keys are pressed
* T09.G3.01: Create a new variable with a descriptive name




ID: T20.G6.02
Topic: T20 – AI Media
Skill: Implement OpenAI Whisper speech recognition
Description: Students use `OpenAI: start recognizing speech in [LANGUAGE v] record as [SOUNDNAME]` as an alternative to Azure speech recognition. They compare Whisper (OpenAI) vs Azure recognition for accuracy, language support, and use cases. Students learn that different AI providers offer similar capabilities with different strengths, preparing them to select appropriate tools for specific requirements.
CSTA: 2-IC-20

Dependencies:
* T20.G6.01: Implement Azure speech recognition with start and end blocks
* T06.G3.02: Build a green-flag script that runs a 3-5 block sequence




ID: T20.G6.03
Topic: T20 – AI Media
Skill: Access recognized speech text with the reporter block
Description: Students use the `text from speech` reporter block to retrieve transcribed text after speech recognition completes. They store the result in variables, display it on screen, or use it as input to other blocks. Students build voice-controlled projects: voice commands trigger actions, spoken answers get checked for correctness, dictated stories get saved. This demonstrates speech as programmatic input.
CSTA: 2-AP-16

Dependencies:
* T20.G6.01: Implement Azure speech recognition with start and end blocks
* T09.G3.01: Create a new variable with a descriptive name
* T06.G3.02: Build a green-flag script that runs a 3-5 block sequence




ID: T20.G6.04
Topic: T20 – AI Media
Skill: Select appropriate language for speech recognition
Description: Students configure speech recognition language to match the spoken language. They experiment with multiple languages (English, Spanish, French, Chinese, etc.) and observe how recognition accuracy drops when language setting doesn't match speech. Students build multilingual projects that support multiple languages via language selection menus, learning to design for diverse users.
CSTA: 2-IC-20

Dependencies:
* T20.G6.01: Implement Azure speech recognition with start and end blocks
* T06.G3.02: Build a green-flag script that runs a 3-5 block sequence




ID: T20.G6.05
Topic: T20 – AI Media
Skill: Debug speech recognition failures
Description: Students troubleshoot common speech recognition issues: (1) microphone not detected—check browser permissions in settings, (2) empty transcription—verify internet connection (cloud-based processing required), (3) wrong language transcribed—match language parameter to spoken language, (4) partial transcription—speak clearly, reduce background noise, use shorter phrases. They build error-checking code to detect empty results and prompt users to try again.
CSTA: 2-AP-17

Dependencies:
* T20.G6.01: Implement Azure speech recognition with start and end blocks
* T08.G4.14: Add else to handle the opposite case
* T06.G3.02: Build a green-flag script that runs a 3-5 block sequence




ID: T20.G6.06
Topic: T20 – AI Media
Skill: Build a voice-controlled interactive project
Description: Students create a project controlled entirely by voice commands: users speak commands like "start," "stop," "next," "help," and the program responds accordingly. They use speech recognition to capture commands, conditional logic to interpret them, and text-to-speech to provide audio feedback. This integrates speech input and output in bidirectional voice interfaces.
CSTA: 2-AP-13

Dependencies:
* T20.G6.03: Access recognized speech text with the reporter block
* T20.G5.05: Implement basic text-to-speech with default parameters
* T08.G4.14: Add else to handle the opposite case
* T06.G3.02: Build a green-flag script that runs a 3-5 block sequence




ID: T20.G6.07
Topic: T20 – AI Media
Skill: Implement text content moderation with the moderation block
Description: Students use `get moderation result for [TEXT]` to check if text content is safe for their application. The block returns "Pass" or "Fail" based on content analysis. Students build content filters: before displaying user-submitted text or AI-generated responses, check moderation results and block unsafe content. This introduces responsible AI practices for text content.
CSTA: 2-IC-23

Dependencies:
* T06.G3.02: Build a green-flag script that runs a 3-5 block sequence
* T08.G4.14: Add else to handle the opposite case
* T09.G3.01: Create a new variable with a descriptive name




ID: T20.G6.08
Topic: T20 – AI Media
Skill: Implement image content moderation with URL moderation block
Description: Students use `get moderation result for image at URL [URL]` to check if images are safe before displaying them. They apply this to AI-generated images or user-uploaded images, checking moderation results before loading images into projects. Students learn that AI-generated content isn't automatically safe and requires verification. This teaches defensive programming for media applications.
CSTA: 2-IC-23

Dependencies:
* T20.G5.01: Generate an AI image with DALL-E using a descriptive prompt
* T06.G3.02: Build a green-flag script that runs a 3-5 block sequence
* T08.G4.14: Add else to handle the opposite case




ID: T20.G6.09
Topic: T20 – AI Media
Skill: Implement costume image moderation
Description: Students use `get moderation result for costume named [NAME]` to check if sprite costumes meet content guidelines. They apply moderation checks to user-created or AI-generated costumes before using them in shared projects. Students build workflows that validate all visual content before publication. This extends moderation concepts to project assets.
CSTA: 2-IC-23

Dependencies:
* T20.G6.08: Implement image content moderation with URL moderation block
* T06.G3.02: Build a green-flag script that runs a 3-5 block sequence




ID: T20.G6.10
Topic: T20 – AI Media
Skill: Implement face detection and write results to a table
Description: Students use `run face detection debug [yes/no] and write into table [TABLE v]` to detect faces in camera input. They set debug to "yes" to visualize detection boxes, then to "no" for production use. Results are written to a table variable containing face positions (x, y coordinates), tilt angle, and facial feature locations (eyes, nose, mouth, ears). Students observe how AI identifies faces in real-time, preparing for face-based interactions.
CSTA: 2-AP-16

Dependencies:
* T10.G4.01: Create a new table variable
* T06.G3.02: Build a green-flag script that runs a 3-5 block sequence
* T09.G3.01: Create a new variable with a descriptive name




ID: T20.G6.11
Topic: T20 – AI Media
Skill: Read face detection data from result tables
Description: Students access face detection results stored in tables: face position coordinates (x, y range from -240 to 240 and -180 to 180), tilt angle, and specific facial feature positions (left_eye_x, left_eye_y, right_eye_x, right_eye_y, nose_x, nose_y, mouth_x, mouth_y, left_ear_x, left_ear_y, right_ear_x, right_ear_y). They iterate through table rows to process multiple detected faces, extracting position data to control sprites or trigger actions based on face locations. This teaches structured data parsing from AI vision outputs.
CSTA: 2-AP-14

Dependencies:
* T20.G6.10: Implement face detection and write results to a table
* T10.G5.02: Iterate through table rows using loops
* T08.G4.14: Add else to handle the opposite case




ID: T20.G6.12
Topic: T20 – AI Media
Skill: Use face position to control sprite movement
Description: Students extract face position coordinates from detection results and use them to control sprite positions on stage. For example, move a sprite to follow the detected face, or create interactive effects positioned around face locations. They map face coordinates (camera space) to stage coordinates (project space), learning coordinate system transformations for vision-based interactions.
CSTA: 2-AP-16

Dependencies:
* T20.G6.11: Read face detection data from result tables
* T06.G3.02: Build a green-flag script that runs a 3-5 block sequence
* T09.G3.01: Create a new variable with a descriptive name




ID: T20.G6.13
Topic: T20 – AI Media
Skill: Implement 2D body part recognition for single or multiple people
Description: Students use `run 2D body part recognition single person [yes/no] table [TABLE v] debug [yes/no]` to detect body keypoints (nose, eyes, ears, shoulders, elbows, wrists, hips, knees, ankles). They set single person to "yes" for tracking one person, "no" for multiple people. Debug mode visualizes skeleton overlay. Results are written to a table containing keypoint names, x/y coordinates, curl angles (180° = straight), and direction angles (0° = pointing up). This introduces pose detection for interactive applications.
CSTA: 2-AP-16

Dependencies:
* T20.G6.10: Implement face detection and write results to a table
* T10.G4.01: Create a new table variable
* T06.G3.02: Build a green-flag script that runs a 3-5 block sequence




ID: T20.G6.14
Topic: T20 – AI Media
Skill: Read body keypoint positions from result tables
Description: Students parse body part recognition results from tables: 17 core body parts (nose, eyes, ears, shoulders, elbows, wrists, hips, knees, ankles) plus 4 aggregate parts (left_arm, right_arm, left_leg, right_leg). Each row has part name, x/y coordinates, curl value, and direction value. They access specific body parts by name or index, extract position data, and use coordinates to track body movement. This teaches structured data extraction from complex AI vision outputs.
CSTA: 2-AP-14

Dependencies:
* T20.G6.13: Implement 2D body part recognition for single or multiple people
* T10.G5.02: Iterate through table rows using loops
* T09.G3.01: Create a new variable with a descriptive name




ID: T20.G6.15
Topic: T20 – AI Media
Skill: Detect specific body poses using keypoint positions
Description: Students implement pose detection logic by comparing keypoint positions: detect raised hand (wrist y-coordinate higher than shoulder), detect jump (both feet off ground baseline), detect crouch (hip y-coordinate below threshold), detect arms spread (wrist x-coordinates far from body center). They use conditional logic to interpret body positions as meaningful actions or states. This demonstrates gesture recognition for interactive controls.
CSTA: 2-AP-13

Dependencies:
* T20.G6.14: Read body keypoint positions from result tables
* T08.G4.14: Add else to handle the opposite case
* T08.G5.08: Combine multiple conditions with AND/OR logic




ID: T20.G6.16
Topic: T20 – AI Media
Skill: Use curl and direction values for arm/leg gesture detection
Description: Students use the curl and dir (direction) values from the body tracking table to detect arm and leg positions. Curl (180° = straight, lower values = bent) helps detect bending motions like arm curls or knee bends. Direction (0° = pointing up, 90° = pointing right) helps detect orientation. They create applications that recognize gestures like arms raised, legs bent, or specific pointing directions based on these angular measurements.
CSTA: 2-AP-13

Dependencies:
* T20.G6.14: Read body keypoint positions from result tables
* T08.G4.14: Add else to handle the opposite case




ID: T20.G6.17
Topic: T20 – AI Media
Skill: Build a gesture-controlled game using body tracking
Description: Students create an interactive game controlled by body movements: jumping raises a character, moving arms left/right steers, crouching triggers special actions. They use continuous body tracking to update game state in real-time, mapping physical movements to game controls. This integrates AI vision with game mechanics for immersive physical computing experiences.
CSTA: 2-AP-16

Dependencies:
* T20.G6.15: Detect specific body poses using keypoint positions
* T06.G3.02: Build a green-flag script that runs a 3-5 block sequence
* T09.G3.01: Create a new variable with a descriptive name




ID: T20.G6.18
Topic: T20 – AI Media
Skill: Debug face and body detection failures
Description: Students troubleshoot common vision AI issues: (1) no faces/bodies detected—check lighting (too dark/bright), camera angle (faces/bodies fully visible), distance (not too far from camera), (2) multiple false detections—adjust confidence thresholds, filter low-confidence results, (3) jittery tracking—implement smoothing by averaging positions over multiple frames, (4) slow performance—reduce camera resolution, limit detection frequency. They build systematic debugging workflows for vision applications.
CSTA: 2-AP-17

Dependencies:
* T20.G6.10: Implement face detection and write results to a table
* T20.G6.13: Implement 2D body part recognition for single or multiple people
* T08.G4.14: Add else to handle the opposite case




ID: T20.G6.19
Topic: T20 – AI Media
Skill: Compare accuracy across different AI vision features
Description: Students test and compare detection accuracy for faces vs body parts under different conditions: lighting (bright, dim, backlit), distance (close, medium, far), angle (front, profile, side), movement (still, slow, fast). They record results in tables and identify which features work best in which conditions. This develops empirical testing skills and understanding of AI limitations.
CSTA: 2-IC-20

Dependencies:
* T20.G6.10: Implement face detection and write results to a table
* T20.G6.13: Implement 2D body part recognition for single or multiple people
* T10.G5.04: Add and remove items from a list




ID: T20.G6.20
Topic: T20 – AI Media
Skill: Integrate multiple AI media features in one application
Description: Students build a comprehensive project using image generation, speech synthesis, speech recognition, content moderation, and vision detection together. For example: a voice-controlled photo booth that generates custom backgrounds (DALL-E), detects user faces, narrates instructions (TTS), accepts voice commands (speech recognition), and filters inappropriate content (moderation). This demonstrates orchestrating multiple AI services in cohesive applications.
CSTA: 2-AP-16

Dependencies:
* T20.G5.01: Generate an AI image with DALL-E using a descriptive prompt
* T20.G5.05: Implement basic text-to-speech with default parameters
* T20.G6.03: Access recognized speech text with the reporter block
* T20.G6.07: Implement text content moderation with the moderation block
* T20.G6.10: Implement face detection and write results to a table


## GRADE 7 (24 skills) - Hand Detection, Neural Networks, Multi-Modal Integration




ID: T20.G7.01
Topic: T20 – AI Media
Skill: Detect and track hand landmarks in real-time
Description: Students use `run hand detection table [TABLE v] debug [yes/no] show video [yes/no]` to detect hands and store landmark coordinates in a table. The table contains 47 rows per hand: rows 1-5 for finger curl/direction data (thumb, index, middle, ring, pinky), rows 6-26 for 21 2D hand keypoints (wrist, thumb_1-4, index_1-4, middle_1-4, ring_1-4, pinky_1-4), and rows 27-47 for 21 3D keypoints with x, y, z coordinates. Debug mode shows visual overlays of detected landmarks.
CSTA: 3A-AP-16

Dependencies:
* T20.G6.13: Implement 2D body part recognition for single or multiple people
* T10.G6.01: Sort a table by a column




ID: T20.G7.02
Topic: T20 – AI Media
Skill: Read finger curl and direction values from hand detection
Description: Students read the first 5 rows of the hand detection table which contain finger data: each row has the finger name (thumb, index, middle, ring, pinky), curl value (180° = straight, lower values = bent/curled), and dir value (0° = pointing up, angles measured clockwise). They use these values to detect finger positions and create applications that respond to finger gestures (e.g., index finger extended vs curled, all fingers straight vs all bent).
CSTA: 3A-DA-09

Dependencies:
* T20.G7.01: Detect and track hand landmarks in real-time
* T08.G5.02: Use a simple if in a script




ID: T20.G7.03
Topic: T20 – AI Media
Skill: Read 2D hand keypoint coordinates for position tracking
Description: Students read rows 6-26 of the hand detection table which contain 21 2D hand keypoints: wrist, thumb_1 through thumb_4, index_1 through index_4, middle_1 through middle_4, ring_1 through ring_4, and pinky_1 through pinky_4. Each row has x and y coordinates. They use these coordinates to track specific hand positions, measure distances between points (e.g., thumb tip to index tip for pinch detection), or create visual effects that follow hand movements.
CSTA: 3A-DA-09

Dependencies:
* T20.G7.02: Read finger curl and direction values from hand detection
* T10.G6.01: Sort a table by a column




ID: T20.G7.04
Topic: T20 – AI Media
Skill: Use 3D hand coordinates for depth-based gestures
Description: Students read rows 27-47 of the hand detection table which contain the same 21 hand keypoints in 3D space with x, y, and z coordinates. The z coordinate represents depth (distance from camera). They use 3D tracking to detect gestures that involve depth, such as hand moving toward/away from camera, creating 3D pointing interfaces, or controlling objects in virtual 3D space based on hand position in all three dimensions.
CSTA: 3A-DA-09

Dependencies:
* T20.G7.03: Read 2D hand keypoint coordinates for position tracking
* T10.G6.01: Sort a table by a column




ID: T20.G7.05
Topic: T20 – AI Media
Skill: Recognize common hand gestures (pinch, fist, open palm)
Description: Students combine data from curl values, direction values, and keypoint positions to recognize common hand gestures. Pinch: thumb and index finger curl both <90° and fingertips close together. Fist: all five fingers curl <90°. Open palm: all five fingers curl >160° and spread apart. Thumbs up: thumb extended (curl >150°) and other fingers curled (<90°). They build reliable gesture recognition with threshold tuning and debouncing to avoid false detections.
CSTA: 3A-DA-09

Dependencies:
* T20.G7.02: Read finger curl and direction values from hand detection
* T20.G7.03: Read 2D hand keypoint coordinates for position tracking




ID: T20.G7.06
Topic: T20 – AI Media
Skill: Create an interactive hand-controlled interface
Description: Students use hand detection to map finger positions to screen coordinates for controlling sprites or UI elements. They track the index finger tip position to move a cursor, implement pinch gestures (thumb-index distance) to trigger actions like clicking or selecting, and use open palm for drag operations. They build complete hand-controlled applications replacing mouse/touch input with gesture recognition.
CSTA: 3A-AP-16

Dependencies:
* T20.G7.05: Recognize common hand gestures (pinch, fist, open palm)
* T11.G6.13: Coordinate multi-sprite interactions




ID: T20.G7.07
Topic: T20 – AI Media
Skill: Detect and visualize 3D body pose from video
Description: Students use `run 3D pose detection debug [yes/no] table [TABLE v]` to detect full body pose with 33 3D keypoints representing all body joints. Unlike 2D body detection, this provides x, y, z coordinates for each point enabling depth-aware pose tracking. They create visual representations by drawing connections between body keypoints to show skeletal structure in 3D space.
CSTA: 3A-DA-09

Dependencies:
* T20.G6.13: Implement 2D body part recognition for single or multiple people
* T10.G6.01: Sort a table by a column




ID: T20.G7.08
Topic: T20 – AI Media
Skill: Build a pose-based activity tracker
Description: Students use 3D pose detection to identify and count specific movements like squats, jumping jacks, or arm raises. They calculate angles between body joints (shoulder-elbow-wrist) to determine exercise positions. They track repetitions by detecting when body returns to starting position and provide real-time feedback with voice announcements using TTS. This combines pose detection with practical fitness applications.
CSTA: 3A-AP-16

Dependencies:
* T20.G7.07: Detect and visualize 3D body pose from video
* T20.G5.05: Implement basic text-to-speech with default parameters




ID: T20.G7.09
Topic: T20 – AI Media
Skill: Implement continuous speech recognition with language selection
Description: Students use `start continuous speech recognition in [LANGUAGE v] into list [LISTNAME v]` to capture ongoing speech and store transcriptions in a list. Unlike single-shot recognition, this streams results continuously—each completed sentence is added to the list while the current sentence updates. They process the list to extract the most recent spoken phrases and stop recognition with `stop continuous speech recognition`.
CSTA: 3A-AP-16

Dependencies:
* T20.G6.01: Implement Azure speech recognition with start and end blocks
* T10.G6.01: Sort a table by a column




ID: T20.G7.10
Topic: T20 – AI Media
Skill: Create a real-time voice-controlled application
Description: Students use continuous speech recognition to capture user commands and map specific phrases to actions using conditional statements. They check if spoken text contains keywords by scanning the recognition list, implement command confirmation ("Did you say 'start'?"), and provide audio and visual feedback to confirm recognized commands before executing actions. This enables hands-free application control.
CSTA: 3A-AP-16

Dependencies:
* T20.G7.09: Implement continuous speech recognition with language selection
* T20.G5.05: Implement basic text-to-speech with default parameters




ID: T20.G7.11
Topic: T20 – AI Media
Skill: Create a neural network model with input and output layers
Description: Students use `create NN model named [NAME]` to initialize a new neural network, then `add layer to NN model [NAME] input shape (SHAPESIZE) output size (OUTPUTSIZE) activation [FUNCTION v]` to define the network structure. They add an input layer with shape matching their data dimensions and an output layer matching the number of classes or predictions. Activation functions include relu (most common for hidden layers), sigmoid (for probability outputs 0-1), tanh, and softmax (for multi-class classification).
CSTA: 3A-AP-17

Dependencies: None




ID: T20.G7.12
Topic: T20 – AI Media
Skill: Add hidden layers to improve neural network accuracy
Description: Students use `add layer to NN model [NAME]` multiple times to create multi-layer networks with hidden layers between input and output. They experiment with different layer sizes (e.g., 128 neurons, 64 neurons, 32 neurons) and activation functions for hidden layers. They understand that deeper networks can learn more complex patterns but may require more training data and longer training time.
CSTA: 3A-AP-17

Dependencies:
* T20.G7.11: Create a neural network model with input and output layers




ID: T20.G7.13
Topic: T20 – AI Media
Skill: Compile neural networks with loss functions and optimizers
Description: Students use `compile NN model [NAME] loss [LOSSFUNCTION v] optimizer [OPTIMIZER v] learning rate (RATE)` to configure the training process. Loss functions include meanSquaredError (for regression/continuous outputs) and categoricalCrossentropy (for classification). Optimizers include adam (adaptive, recommended for most tasks), sgd (stochastic gradient descent, basic), and adagrad (adaptive gradient). Learning rate typically ranges from 0.001 to 0.1 (lower = slower but more stable learning).
CSTA: 3A-AP-17

Dependencies:
* T20.G7.12: Add hidden layers to improve neural network accuracy




ID: T20.G7.14
Topic: T20 – AI Media
Skill: Train neural networks using tabular data
Description: Students use `train NN model [NAME] using table [TABLENAME v] rows from [STARTROW] to [ENDROW] input columns [INPUTCOLUMNS] output column [OUTPUTCOLUMN] batch size [BATCHSIZE] epochs [EPOCHS]` to train the model on prepared data. Each row in the table is one training sample. INPUTCOLUMNS is comma-separated (e.g., "pixel1,pixel2,pixel3" or "feature1,feature2"). They set epochs (10-50 training rounds) and batch size (10-32 samples processed together), then observe training progress.
CSTA: 3A-AP-17

Dependencies:
* T20.G7.13: Compile neural networks with loss functions and optimizers
* T10.G6.01: Sort a table by a column




ID: T20.G7.15
Topic: T20 – AI Media
Skill: Make predictions using trained neural networks
Description: Students use `predict using NN model [NAME] for table [TABLENAME v] rows from [STARTROW] to [ENDROW] input columns [INPUTCOLUMNS] output column [OUTPUTCOLUMN]` to classify new data using their trained neural network. The block reads input data from the table, runs it through the neural network, and writes predictions to the output column. They interpret prediction results (for classification: class labels; for regression: numeric values).
CSTA: 3A-AP-17

Dependencies:
* T20.G7.14: Train neural networks using tabular data




ID: T20.G7.16
Topic: T20 – AI Media
Skill: Save and load trained neural network models
Description: Students use `save NN model named [NAME]` to persist trained models on the CreatiCode server, and `load NN model named [NAME]` to retrieve them later without retraining. They test that loaded models produce the same predictions as before saving. Saved models retain their architecture, weights, and compilation settings. This enables model reuse and sharing.
CSTA: 3A-AP-17

Dependencies:
* T20.G7.15: Make predictions using trained neural networks




ID: T20.G7.17
Topic: T20 – AI Media
Skill: Create KNN classifiers from training data
Description: Students use `create KNN number classifier from table [TABLE v] K [K] named [NAME]` to build a K-Nearest Neighbors classifier from labeled examples. They prepare a training data table with a 'label' column (the class to predict) and numeric property columns (features). They choose appropriate K value (typically 3-7: smaller K is more sensitive to noise, larger K is smoother but may miss patterns).
CSTA: 3A-AP-17

Dependencies:
* T10.G6.01: Sort a table by a column




ID: T20.G7.18
Topic: T20 – AI Media
Skill: Make predictions with KNN and visualize neighbors
Description: Students use `predict for table [TABLE v] with classifier [NAME] show neighbors [yes/no]` to classify new data points. The block writes predicted labels to the 'label' column and optionally shows indices of the K nearest neighbors. They analyze which neighbors influenced each prediction to understand the classifier's decision-making. They compare KNN (fast training, transparent decisions) with neural networks (better for complex patterns).
CSTA: 3A-AP-17

Dependencies:
* T20.G7.17: Create KNN classifiers from training data




ID: T20.G7.19
Topic: T20 – AI Media
Skill: Build a gesture classifier using hand landmarks and KNN
Description: Students collect hand landmark data for different gestures using hand detection and store features (finger curl values, finger directions, keypoint positions) in a training table. They create a KNN classifier from the gesture dataset with appropriate K value. Then they predict gestures in real-time by feeding current hand landmarks into the classifier, enabling custom gesture recognition beyond the built-in gestures.
CSTA: 3A-AP-17

Dependencies:
* T20.G7.05: Recognize common hand gestures (pinch, fist, open palm)
* T20.G7.18: Make predictions with KNN and visualize neighbors




ID: T20.G7.20
Topic: T20 – AI Media
Skill: Combine ChatGPT vision with image capture for scene description
Description: Students use `attach costume [NAME] to chat` to send images to ChatGPT, then request descriptions, analysis, or answers about the image content. They capture camera snapshots as costumes, attach them to ChatGPT requests, and receive detailed text descriptions of what's in the image. This demonstrates multi-modal AI where visual input produces text output.
CSTA: 3A-AP-17

Dependencies:
* T20.G5.16: Use ChatGPT to generate image descriptions for DALL-E
* T06.G4.01: Use broadcast to coordinate sprite actions




ID: T20.G7.21
Topic: T20 – AI Media
Skill: Build an AI-powered object identifier
Description: Students use camera detection to capture objects, attach the image to ChatGPT with `attach costume [NAME] to chat`, and ask "What object is in this image?". They process ChatGPT's response to extract object name and key characteristics, then announce results with text-to-speech. They create an interactive application that identifies and provides information about objects shown to the camera.
CSTA: 3A-AP-17

Dependencies:
* T20.G7.20: Combine ChatGPT vision with image capture for scene description
* T20.G5.05: Implement basic text-to-speech with default parameters




ID: T20.G7.22
Topic: T20 – AI Media
Skill: Create an accessible image narrator for visually impaired users
Description: Students build a system that captures images via camera, sends them to ChatGPT for detailed description, and converts the description to speech automatically. They use detailed prompts requesting scene descriptions ("Describe everything you see in this image in detail"), then speak the response. They design simple voice commands to trigger the image capture and narration cycle, creating an accessibility tool.
CSTA: 3A-IC-24

Dependencies:
* T20.G7.21: Build an AI-powered object identifier
* T20.G7.10: Create a real-time voice-controlled application




ID: T20.G7.23
Topic: T20 – AI Media
Skill: Integrate multiple AI models in a single application
Description: Students combine hand detection, speech recognition, and neural network/KNN predictions in one cohesive project where each AI component serves a distinct purpose. They coordinate timing so that AI models run sequentially or in parallel without conflicts. They create a unified user interface that clearly shows which AI system is active and what it's processing.
CSTA: 3A-AP-17

Dependencies:
* T20.G7.19: Build a gesture classifier using hand landmarks and KNN
* T20.G7.10: Create a real-time voice-controlled application
* T20.G7.15: Make predictions using trained neural networks




ID: T20.G7.24
Topic: T20 – AI Media
Skill: Optimize AI model performance through parameter tuning
Description: Students systematically test different neural network configurations (layer sizes, learning rates, epochs, batch sizes) to improve accuracy. They document performance metrics for each configuration in a table, then compare validation accuracy across experiments to identify optimal parameters. They learn to balance accuracy versus training time and overfitting risk.
CSTA: 3A-AP-17

Dependencies:
* T20.G7.14: Train neural networks using tabular data
* T10.G6.01: Sort a table by a column


## GRADE 8 (28 skills) - Deep Learning, Semantic Search, Production AI Systems, Accessibility




ID: T20.G8.01
Topic: T20 – AI Media
Skill: Analyze text structure using NLP sentence analysis
Description: Students use `analyze sentence [SENTENCE] and write into table [TABLENAME v]` to perform natural language processing using Google Natural Language API. The resulting table has 7 columns: TEXT (each word), LEMMA (word stem, e.g., "running"→"run"), TYPE (noun, verb, adjective, etc.), PERSON (first/second/third for pronouns), OFFSET (position in sentence), LABEL (detailed grammatical function), DEPENDS (row number of word this depends on). They explore how computers understand language structure.
CSTA: 3B-DA-05

Dependencies:
* T10.G6.01: Sort a table by a column




ID: T20.G8.02
Topic: T20 – AI Media
Skill: Extract key information from text using NLP analysis
Description: Students use NLP sentence analysis to identify and extract key information: nouns (subjects, objects), verbs (actions), adjectives (descriptions), and their grammatical relationships. They create applications that summarize text, extract keywords, or identify the main topic. They use the DEPENDS column to trace grammatical dependencies between words.
CSTA: 3B-DA-05

Dependencies:
* T20.G8.01: Analyze text structure using NLP sentence analysis




ID: T20.G8.03
Topic: T20 – AI Media
Skill: Create semantic search databases from text collections
Description: Students use `create semantic database from table [TABLE v]` to build a vector database using Pinecone. They prepare a table with a 'key' column (text to be searchable, e.g., FAQ questions, product descriptions, document excerpts) and optional metadata columns (category, date, author). They understand that semantic search works by converting text to embeddings (vector representations) that capture meaning, enabling similarity-based search. Only one database per project is supported.
CSTA: 3B-DA-05

Dependencies:
* T10.G6.01: Sort a table by a column




ID: T20.G8.04
Topic: T20 – AI Media
Skill: Search with semantic similarity and ranking
Description: Students use `search semantic database with [QUERY] store top (K) in table [TABLE v]` to find semantically similar entries. The block converts the query to an embedding vector and finds the K most similar records from the database. Results include a similarity score (0-1 scale where higher = more similar, typically >0.7 is considered relevant). Unlike keyword search, semantic search finds results based on meaning, so "What's your phone number?" matches "Contact: 555-1234".
CSTA: 3B-DA-05

Dependencies:
* T20.G8.03: Create semantic search databases from text collections




ID: T20.G8.05
Topic: T20 – AI Media
Skill: Filter semantic search results with conditions
Description: Students use `search semantic database with [QUERY] where [CONDITION] store top (K) in table [TABLE v]` to perform semantic searches with SQL-like filtering on metadata. Example conditions: "category='science' and date>='2024-01-01'" or "author='Smith'". They combine semantic similarity with structured filtering to narrow results to specific categories, date ranges, or other criteria.
CSTA: 3B-DA-05

Dependencies:
* T20.G8.04: Search with semantic similarity and ranking
* T10.G6.02: Filter table rows based on a condition




ID: T20.G8.06
Topic: T20 – AI Media
Skill: Build a question-answering system with semantic search
Description: Students create a semantic database from informational texts or FAQs, then use semantic search to find relevant passages for user queries. They combine search results with ChatGPT: send the question and retrieved context to ChatGPT for synthesis into a natural language answer. They display both the source passages and the generated answer for transparency. This demonstrates the RAG (Retrieval-Augmented Generation) pattern.
CSTA: 3B-AP-16

Dependencies:
* T20.G8.04: Search with semantic similarity and ranking
* T20.G5.16: Use ChatGPT to generate image descriptions for DALL-E




ID: T20.G8.07
Topic: T20 – AI Media
Skill: Design deep neural networks with multiple hidden layers
Description: Students build neural networks with 3+ hidden layers using varied architectures (e.g., 128-64-32 neurons progressively decreasing). They experiment with different activation functions at each layer and analyze how network depth affects learning capacity and training time. They learn that deeper networks can learn more complex patterns but may overfit on small datasets.
CSTA: 3B-AP-16

Dependencies:
* T20.G7.12: Add hidden layers to improve neural network accuracy




ID: T20.G8.08
Topic: T20 – AI Media
Skill: Evaluate neural network performance with test data splits
Description: Students manually split their data into training and test sets (e.g., 80% training, 20% testing). They train only on training data and evaluate accuracy on test data to measure generalization. They detect overfitting (when training accuracy is high but test accuracy is low) and adjust model complexity accordingly. This teaches proper evaluation methodology.
CSTA: 3B-DA-07

Dependencies:
* T20.G7.14: Train neural networks using tabular data
* T10.G6.01: Sort a table by a column




ID: T20.G8.09
Topic: T20 – AI Media
Skill: Build a neural network for digit or pattern classification
Description: Students create and train a neural network to recognize handwritten digits or simple patterns. They prepare training data (table with pixel values or features as inputs and labels as outputs), design appropriate architecture (e.g., 784 inputs for 28x28 images → 128 hidden → 10 outputs for digits 0-9), train with sufficient epochs, and test on new examples. They build an interface where users can draw for real-time recognition.
CSTA: 3B-AP-16

Dependencies:
* T20.G7.16: Save and load trained neural network models
* T10.G6.01: Sort a table by a column




ID: T20.G8.10
Topic: T20 – AI Media
Skill: Compare neural network vs KNN for classification tasks
Description: Students implement both neural network and KNN classifiers for the same classification task and compare their performance. They measure accuracy, training time, prediction speed, and interpretability. They identify when each approach works best: KNN for small datasets and transparent decisions, neural networks for complex patterns and large datasets. They document trade-offs in a comparison table.
CSTA: 3B-DA-07

Dependencies:
* T20.G7.16: Save and load trained neural network models
* T20.G7.18: Make predictions with KNN and visualize neighbors




ID: T20.G8.11
Topic: T20 – AI Media
Skill: Optimize hyperparameters through systematic experimentation
Description: Students design experiments that test multiple combinations of learning rates (0.001, 0.01, 0.1), batch sizes (8, 16, 32), epoch counts (10, 25, 50), and network architectures. They create tables to log all hyperparameters and their corresponding test accuracy. They use the logged data to identify patterns and select optimal configurations for their specific task.
CSTA: 3B-DA-07

Dependencies:
* T20.G7.24: Optimize AI model performance through parameter tuning




ID: T20.G8.12
Topic: T20 – AI Media
Skill: Implement transfer learning by adapting pre-trained models
Description: Students load a pre-trained neural network model using `load NN model named [NAME]`, then fine-tune it on a new related task by continuing training with new data. They use lower learning rates to preserve learned features while adapting to new patterns. They compare training time and accuracy with models trained from scratch, demonstrating that transfer learning enables faster training with less data.
CSTA: 3B-AP-16

Dependencies:
* T20.G7.16: Save and load trained neural network models
* T20.G7.14: Train neural networks using tabular data




ID: T20.G8.13
Topic: T20 – AI Media
Skill: Build real-time pose-based interactive games
Description: Students create interactive games controlled entirely by body movements using 3D pose detection. Examples include dance games (match target poses for points), fitness games (track and count exercises), or action games (dodge obstacles by moving, jump by raising arms). They implement scoring systems, real-time feedback, and level progression based on pose accuracy and performance.
CSTA: 3B-AP-16

Dependencies:
* T20.G7.08: Build a pose-based activity tracker
* T20.G7.07: Detect and visualize 3D body pose from video




ID: T20.G8.14
Topic: T20 – AI Media
Skill: Create advanced gesture control with hand orientation
Description: Students use 3D hand detection to track complete hand orientation (rotation, tilt) by analyzing spatial relationships between landmarks. They implement rotation gestures (turn hand to rotate objects), pinch-and-zoom with both hands, and directional swipes. They build 3D object manipulation interfaces controlled entirely by hand gestures, enabling touchless interaction with virtual objects.
CSTA: 3B-AP-16

Dependencies:
* T20.G7.06: Create an interactive hand-controlled interface
* T20.G7.04: Use 3D hand coordinates for depth-based gestures




ID: T20.G8.15
Topic: T20 – AI Media
Skill: Build a custom sign language recognition system
Description: Students build a dataset of sign language gestures using hand detection landmarks, including hand shape, position, and movement trajectories over time. They train a classifier (KNN or neural network) on the gesture dataset with multiple examples per sign. They create an application that recognizes basic sign language and displays the corresponding text or speaks it aloud, creating an accessibility tool.
CSTA: 3B-IC-24

Dependencies:
* T20.G7.19: Build a gesture classifier using hand landmarks and KNN
* T20.G8.07: Design deep neural networks with multiple hidden layers




ID: T20.G8.16
Topic: T20 – AI Media
Skill: Build multimodal AI systems with synchronized inputs
Description: Students create applications that process video (pose/hand detection), audio (speech recognition), and visual analysis (ChatGPT vision) simultaneously. They synchronize timestamps across different AI inputs to coordinate responses. They implement systems where gestures, spoken commands, and visual context all influence application behavior, creating rich multimodal interactions.
CSTA: 3B-AP-16

Dependencies:
* T20.G7.23: Integrate multiple AI models in a single application
* T20.G7.09: Implement continuous speech recognition with language selection




ID: T20.G8.17
Topic: T20 – AI Media
Skill: Implement intelligent caching for AI performance
Description: Students design systems that cache AI predictions for repeated inputs to reduce processing time. They store gesture classifications, speech recognition results, or neural network predictions in tables for quick lookup. They implement cache invalidation (when to refresh cached data) and compare performance with and without caching to measure improvement.
CSTA: 3B-AP-16

Dependencies:
* T20.G7.23: Integrate multiple AI models in a single application
* T10.G6.01: Sort a table by a column




ID: T20.G8.18
Topic: T20 – AI Media
Skill: Create AI-powered content moderation systems
Description: Students build comprehensive moderation systems combining multiple checks: text moderation for language filtering, image moderation for visual content, and ChatGPT vision for context analysis. They implement confidence thresholds where borderline content is flagged for human review. They log all moderation decisions with timestamps and reasoning for transparency and auditing.
CSTA: 3B-IC-28

Dependencies:
* T20.G6.07: Implement text content moderation with the moderation block
* T20.G6.08: Implement image content moderation with URL moderation block
* T20.G7.20: Combine ChatGPT vision with image capture for scene description




ID: T20.G8.19
Topic: T20 – AI Media
Skill: Design personalized AI learning assistants
Description: Students create educational tools that use semantic search to find relevant learning materials based on student questions, combined with ChatGPT to provide explanations tailored to comprehension level. They track which topics students ask about most frequently to identify knowledge gaps. They implement speech input and output for hands-free learning, making the assistant accessible.
CSTA: 3B-AP-16

Dependencies:
* T20.G8.06: Build a question-answering system with semantic search
* T20.G7.10: Create a real-time voice-controlled application




ID: T20.G8.20
Topic: T20 – AI Media
Skill: Build AI systems with explainable outputs
Description: Students create applications that not only make predictions but also explain their reasoning. For KNN: show which neighbors influenced the decision. For neural networks: display confidence scores for each class. For semantic search: show similarity scores and matched phrases. They implement "explain" buttons that reveal the reasoning process, helping users understand and trust AI decisions.
CSTA: 3B-IC-28

Dependencies:
* T20.G7.18: Make predictions with KNN and visualize neighbors
* T20.G8.08: Evaluate neural network performance with test data splits




ID: T20.G8.21
Topic: T20 – AI Media
Skill: Implement real-time AI analytics dashboards
Description: Students create visual dashboards that display ongoing AI system performance metrics including prediction accuracy, processing speed, and confidence levels. They update visualizations in real-time as the AI processes new inputs. They track and display historical performance data to identify trends or degradation over time, enabling monitoring of AI system health.
CSTA: 3B-DA-06

Dependencies:
* T20.G8.20: Build AI systems with explainable outputs
* T10.G6.01: Sort a table by a column




ID: T20.G8.22
Topic: T20 – AI Media
Skill: Develop AI-powered video analysis tools
Description: Students combine pose detection, hand tracking, and ChatGPT vision to analyze video content and extract insights. They process video frames sequentially to track changes over time and identify patterns or events (e.g., "user raised hand 5 times," "object appeared at timestamp X"). They create summaries or highlight reels based on detected activities.
CSTA: 3B-DA-05

Dependencies:
* T20.G8.13: Build real-time pose-based interactive games
* T20.G8.14: Create advanced gesture control with hand orientation
* T20.G7.20: Combine ChatGPT vision with image capture for scene description




ID: T20.G8.23
Topic: T20 – AI Media
Skill: Create adaptive AI systems that improve from user feedback
Description: Students build applications that collect user feedback on AI predictions (correct/incorrect) and store this data for model retraining. They implement mechanisms to periodically retrain models with accumulated feedback. They display metrics showing accuracy improvement over time as more feedback is collected, demonstrating continuous learning.
CSTA: 3B-DA-07

Dependencies:
* T20.G8.08: Evaluate neural network performance with test data splits
* T20.G7.14: Train neural networks using tabular data




ID: T20.G8.24
Topic: T20 – AI Media
Skill: Build production-ready AI error handling systems
Description: Students implement comprehensive error handling for AI operations: camera access failures (prompt for permissions, provide alternative input), model loading errors (retry with backoff, show status), unexpected prediction outputs (validate results, provide fallback behavior). They create fallback behaviors when AI components fail (e.g., manual control when gesture detection fails) and log errors with context for debugging.
CSTA: 3B-AP-21

Dependencies:
* T20.G8.16: Build multimodal AI systems with synchronized inputs
* T12.G6.01: Trace complex code with multiple variables




ID: T20.G8.25
Topic: T20 – AI Media
Skill: Optimize AI model deployment for resource constraints
Description: Students compare model performance versus computational cost by measuring prediction time and memory usage for different architectures. They implement model compression techniques like reducing layer sizes while maintaining acceptable accuracy. They create lightweight versions of models suitable for real-time applications on devices with limited resources.
CSTA: 3B-AP-17

Dependencies:
* T20.G8.11: Optimize hyperparameters through systematic experimentation
* T20.G8.07: Design deep neural networks with multiple hidden layers




ID: T20.G8.26
Topic: T20 – AI Media
Skill: Design ethical AI systems with bias detection
Description: Students analyze training datasets for representation imbalances across different groups (gender, age, ethnicity in images; topics and perspectives in text). They test AI models with diverse inputs to identify performance disparities. They implement fairness metrics that measure and report model performance across different demographic groups, identifying and documenting bias in their AI systems.
CSTA: 3B-IC-28

Dependencies:
* T20.G8.08: Evaluate neural network performance with test data splits
* T20.G8.02: Extract key information from text using NLP analysis




ID: T20.G8.27
Topic: T20 – AI Media
Skill: Create comprehensive AI accessibility platforms
Description: Students build integrated platforms combining image narration, sign language recognition, voice control, and text-to-speech to serve users with diverse accessibility needs. They implement user profiles that remember preferred interaction modes and AI settings. They design systems that can switch between accessibility features seamlessly based on context and user preference.
CSTA: 3B-IC-24

Dependencies:
* T20.G8.15: Build a custom sign language recognition system
* T20.G7.22: Create an accessible image narrator for visually impaired users
* T20.G8.19: Design personalized AI learning assistants




ID: T20.G8.28
Topic: T20 – AI Media
Skill: Develop end-to-end AI media production pipelines
Description: Students create complete workflows from data collection (camera, speech, sensors) through multiple AI stages (detection, classification, generation) to polished outputs (videos, reports, interactive experiences). They implement quality control checkpoints at each stage. They document the entire pipeline with performance metrics, identifying bottlenecks and optimization opportunities. This capstone skill integrates all AI media concepts.
CSTA: 3B-AP-16

Dependencies:
* T20.G8.22: Develop AI-powered video analysis tools
* T20.G8.21: Implement real-time AI analytics dashboards
* T20.G8.24: Build production-ready AI error handling systems


