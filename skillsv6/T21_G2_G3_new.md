# T21 – Chatbots & Prompting: Grade 2 and Grade 3 Skills

## Grade 2 (G2): Advanced Prompting Foundations (12 skills)

### Role-Task-Format Prompt Structure

**ID:** T21.G2.01
**Topic:** T21 – Chatbots & Prompting
**Skill:** Identify the three parts of a structured prompt (Role, Task, Format)
**Description:** Students learn that effective prompts have three components: telling the chatbot what role to take (e.g., "You are a teacher"), what task to do (e.g., "explain photosynthesis"), and what format to use (e.g., "in simple words"). They analyze example prompts to identify these three parts using picture-based organizers. Assessment: Given four complete prompts, student correctly identifies and labels the Role, Task, and Format components in at least three prompts, using color coding or numbered labels to show each part.

**Dependencies:**
* T21.G1.02: Add specific details to questions to get better answers
* T21.G1.07: Use complete sentences when asking chatbot questions

---

**ID:** T21.G2.02
**Topic:** T21 – Chatbots & Prompting
**Skill:** Create simple prompts using Role-Task-Format structure
**Description:** Students construct basic prompts that include all three components: Role, Task, and Format. They use sentence frames and graphic organizers with pictures to build complete structured prompts (e.g., "You are a scientist [role]. Tell me how plants grow [task] in a short list [format]"). Assessment: Student independently creates at least two structured prompts on different topics, with each prompt clearly including all three components (Role, Task, Format), and tests them with a chatbot to verify they work as intended.

**Dependencies:**
* T21.G2.01: Identify the three parts of a structured prompt (Role, Task, Format)

---

**ID:** T21.G2.03
**Topic:** T21 – Chatbots & Prompting
**Skill:** Compare prompts with and without structure
**Description:** Students test pairs of prompts—one with Role-Task-Format structure and one without—to observe differences in chatbot responses. They evaluate which version produces clearer, more useful answers. Assessment: Given a basic question (e.g., "Tell me about volcanoes"), student creates both an unstructured version and a structured version with Role-Task-Format, asks both to a chatbot, and explains which response better meets their needs and why.

**Dependencies:**
* T21.G2.02: Create simple prompts using Role-Task-Format structure
* T21.G1.04: Check chatbot answers using other sources

---

### Algorithm Design for Prompts

**ID:** T21.G2.04
**Topic:** T21 – Chatbots & Prompting
**Skill:** Plan a sequence of prompts to accomplish a goal
**Description:** Students design a step-by-step plan (algorithm) for using multiple prompts to achieve a specific goal, such as learning about a topic or solving a problem. They use flowcharts with pictures to map out their prompt sequence (e.g., Step 1: Ask for definition, Step 2: Ask for examples, Step 3: Ask for explanation). Assessment: Student creates a 3-4 step prompt sequence plan using a visual organizer to research a topic (e.g., ocean animals), shows the sequence with arrows, and successfully executes the plan with a chatbot.

**Dependencies:**
* T21.G1.03: Ask follow-up questions based on chatbot responses
* T21.G2.02: Create simple prompts using Role-Task-Format structure

---

**ID:** T21.G2.05
**Topic:** T21 – Chatbots & Prompting
**Skill:** Revise prompt sequences based on results
**Description:** Students learn to evaluate their prompt sequence results and make improvements. When a step doesn't produce the desired information, they identify which prompt needs revision and create an improved version. They understand iteration as part of the prompting process. Assessment: Student executes a 3-step prompt sequence, identifies at least one prompt that didn't work well, explains what was wrong (e.g., "too vague" or "didn't ask for examples"), and creates an improved version that produces better results.

**Dependencies:**
* T21.G2.04: Plan a sequence of prompts to accomplish a goal
* T21.G1.05: Identify when a chatbot answer is incomplete

---

### Sophisticated Evaluation Skills

**ID:** T21.G2.06
**Topic:** T21 – Chatbots & Prompting
**Skill:** Evaluate response quality using multiple criteria
**Description:** Students assess chatbot responses using specific criteria: completeness (answers the whole question), accuracy (facts are correct), clarity (easy to understand), and usefulness (helps with their goal). They use simple rubrics with picture scales (e.g., thumbs up/middle/down) for each criterion. Assessment: Given three chatbot responses to the same question, student evaluates each using all four criteria (completeness, accuracy, clarity, usefulness), selects the best response, and explains their choice using at least two criteria.

**Dependencies:**
* T21.G1.04: Check chatbot answers using other sources
* T21.G1.05: Identify when a chatbot answer is incomplete
* T21.G1.10: Notice patterns in how chatbots structure their responses

---

**ID:** T21.G2.07
**Topic:** T21 – Chatbots & Prompting
**Skill:** Identify bias or missing perspectives in responses
**Description:** Students recognize when a chatbot response presents only one viewpoint or leaves out important perspectives. They learn to ask questions like "Are there other ways to think about this?" or "What else should I know?" to get more complete information. Assessment: When given a chatbot response about a topic with multiple perspectives (e.g., "Should people have pets?"), student identifies that only one viewpoint is presented, formulates a question to get alternative perspectives, and compares the two responses.

**Dependencies:**
* T21.G2.06: Evaluate response quality using multiple criteria
* T21.G1.06: Recognize when to ask a person instead of a chatbot

---

### Comparison and Testing Concepts

**ID:** T21.G2.08
**Topic:** T21 – Chatbots & Prompting
**Skill:** Test the same prompt with different chatbots and compare results
**Description:** Students learn that different chatbots may respond differently to the same prompt. They conduct simple experiments asking the same question to different chatbots (or the same chatbot at different times) and compare the responses for similarities and differences. Assessment: Student asks the same structured prompt to two different chatbots, documents both responses using screenshots or written notes, and identifies at least two similarities and two differences between the responses.

**Dependencies:**
* T21.G2.02: Create simple prompts using Role-Task-Format structure
* T21.GK.04: Observe that similar questions get similar answers

---

**ID:** T21.G2.09
**Topic:** T21 – Chatbots & Prompting
**Skill:** Change one element of a prompt to test its effect
**Description:** Students conduct controlled experiments by changing only one part of a prompt (e.g., only the role, or only the format) while keeping other parts the same. They observe how each change affects the response, building understanding of cause and effect in prompting. Assessment: Student creates a baseline prompt, then creates three variations each changing only one element (role, task, OR format), tests all four versions, and explains how each change affected the chatbot's response.

**Dependencies:**
* T21.G2.03: Compare prompts with and without structure
* T21.G2.08: Test the same prompt with different chatbots and compare results

---

### Preparing for Coding Concepts

**ID:** T21.G2.10
**Topic:** T21 – Chatbots & Prompting
**Skill:** Recognize that prompts are instructions for the chatbot
**Description:** Students understand that prompts function as instructions or commands that tell the chatbot what to do, similar to how they might give instructions to a friend or robot. They learn that clear, specific instructions produce better results. Assessment: Student explains the connection between giving instructions to a person/robot and writing prompts for a chatbot, provides at least two examples of how unclear instructions lead to poor results, and demonstrates improving an unclear prompt to make it more instructive.

**Dependencies:**
* T21.G2.01: Identify the three parts of a structured prompt (Role, Task, Format)
* T21.G2.04: Plan a sequence of prompts to accomplish a goal

---

**ID:** T21.G2.11
**Topic:** T21 – Chatbots & Prompting
**Skill:** Use picture-based organizers to design complex prompts
**Description:** Students use visual tools (graphic organizers, flowcharts, planning sheets with picture prompts) to design more complex prompts that include multiple requirements or steps. They learn to break down what they want into organized components before writing the prompt. Assessment: Student uses a provided graphic organizer to design a complex prompt that includes at least four specific requirements (e.g., topic, role, format, length, tone), then successfully writes and tests the complete prompt with a chatbot.

**Dependencies:**
* T21.G2.02: Create simple prompts using Role-Task-Format structure
* T21.G2.04: Plan a sequence of prompts to accomplish a goal

---

**ID:** T21.G2.12
**Topic:** T21 – Chatbots & Prompting
**Skill:** Document and share successful prompts with others
**Description:** Students learn to record prompts that worked well, along with notes about what made them successful, creating a personal prompt library. They practice sharing effective prompts with classmates and explaining why they work. Assessment: Student creates a collection of at least three successful prompts on different topics, documents what made each one effective (using labels like "specific details," "clear format," "good role"), and teaches a peer how to use one of their prompts.

**Dependencies:**
* T21.G2.06: Evaluate response quality using multiple criteria
* T21.G2.11: Use picture-based organizers to design complex prompts

---

## Grade 3 (G3): Introduction to ChatGPT Coding (15 skills)

### First ChatGPT Coding Blocks

**ID:** T21.G3.01
**Topic:** T21 – Chatbots & Prompting
**Skill:** Use the basic ChatGPT request block with default settings
**Description:** Students learn to use their first ChatGPT coding block: `OpenAI ChatGPT: request [PROMPT] result [VARIABLE] mode [waiting] length [100] temperature [0.7] session [new chat]`. They understand that this block sends a prompt to ChatGPT and stores the response in a variable. They practice with simple prompts and observe the results. Assessment: Student creates a program with at least three different ChatGPT request blocks using different prompts, stores each result in a different variable, and uses the "say" block to display each response, demonstrating understanding of variable storage.

**Dependencies:**
* T21.G2.02: Create simple prompts using Role-Task-Format structure
* T21.G2.10: Recognize that prompts are instructions for the chatbot

---

**ID:** T21.G3.02
**Topic:** T21 – Chatbots & Prompting
**Skill:** Store and display ChatGPT responses using variables
**Description:** Students learn to create variables to store ChatGPT responses and use those variables in their programs. They understand that the response text is saved in the variable and can be used multiple times (displayed, checked, used in other blocks). Assessment: Student creates a program that requests a ChatGPT response, stores it in a descriptive variable name (e.g., "animalFact"), and then uses that variable in at least two different ways (e.g., displays it with "say" and checks its length or saves it to a list).

**Dependencies:**
* T21.G3.01: Use the basic ChatGPT request block with default settings

---

**ID:** T21.G3.03
**Topic:** T21 – Chatbots & Prompting
**Skill:** Understand the difference between "waiting" and "streaming" modes
**Description:** Students experiment with both mode options in the ChatGPT block: "waiting" (program waits for complete response before continuing) and "streaming" (response appears word-by-word in real-time). They identify when each mode is appropriate based on their program's needs. Assessment: Student creates two versions of the same ChatGPT program—one using waiting mode and one using streaming mode—demonstrates both to a peer, and explains when they would use each mode (e.g., "Use waiting when I need the full answer before the next step").

**Dependencies:**
* T21.G3.01: Use the basic ChatGPT request block with default settings

---

### System Prompts and Session Management

**ID:** T21.G3.04
**Topic:** T21 – Chatbots & Prompting
**Skill:** Use the system request block to set chatbot behavior
**Description:** Students learn to use `OpenAI ChatGPT: system request [PROMPT] session [new chat] result [VARIABLE] temperature [0.7]` to establish persistent behavior for the chatbot (e.g., "You are a helpful science teacher who explains concepts to third graders"). They understand that system prompts set the "personality" or role for the entire conversation. Assessment: Student creates a program with a system request that establishes a specific role, then uses regular request blocks to ask questions, demonstrating that the chatbot maintains the assigned role across multiple questions.

**Dependencies:**
* T21.G3.01: Use the basic ChatGPT request block with default settings
* T21.G2.01: Identify the three parts of a structured prompt (Role, Task, Format)

---

**ID:** T21.G3.05
**Topic:** T21 – Chatbots & Prompting
**Skill:** Manage conversations using session types (new chat vs. continue)
**Description:** Students learn the difference between "new chat" (starts fresh conversation, chatbot doesn't remember previous messages) and "continue" (maintains conversation history). They practice using both session types appropriately to create connected conversations or start fresh when needed. Assessment: Student creates a program demonstrating both session types: one section with "continue" showing the chatbot remembering context across 3+ messages, and another section using "new chat" to start a different topic, explaining when to use each type.

**Dependencies:**
* T21.G3.01: Use the basic ChatGPT request block with default settings
* T21.G1.03: Ask follow-up questions based on chatbot responses

---

**ID:** T21.G3.06
**Topic:** T21 – Chatbots & Prompting
**Skill:** Select and switch between multiple ChatGPT bots
**Description:** Students learn to use `select ChatGPT bot [1/2/3/4]` to manage multiple separate conversations simultaneously. They understand that different bot numbers maintain independent conversation histories, allowing them to have parallel conversations about different topics. Assessment: Student creates a program that uses at least two different ChatGPT bot numbers (e.g., bot 1 for math help, bot 2 for story writing), demonstrates that each bot maintains its own conversation thread without mixing topics, and explains when using multiple bots is useful.

**Dependencies:**
* T21.G3.05: Manage conversations using session types (new chat vs. continue)

---

### Multi-Turn Conversations and Context

**ID:** T21.G3.07
**Topic:** T21 – Chatbots & Prompting
**Skill:** Build a multi-turn conversation with connected prompts
**Description:** Students create programs where each prompt builds on previous responses, creating a coherent conversation flow. They learn to reference earlier information and maintain topic continuity using "continue" session mode. Assessment: Student creates a conversation program of at least 5 turns where each prompt logically follows from the previous response (e.g., ask about a topic, ask for more details, ask for examples, ask for explanation, ask how to apply it), demonstrating conversation continuity.

**Dependencies:**
* T21.G3.05: Manage conversations using session types (new chat vs. continue)
* T21.G2.04: Plan a sequence of prompts to accomplish a goal

---

**ID:** T21.G3.08
**Topic:** T21 – Chatbots & Prompting
**Skill:** Use context from previous responses in new prompts
**Description:** Students learn to reference information from earlier chatbot responses in their new prompts (e.g., "You said that plants need sunlight. How much sunlight do tomatoes need?"). They understand how maintaining context makes conversations more natural and efficient. Assessment: Student creates a program where at least two prompts explicitly reference information from previous ChatGPT responses (e.g., "Tell me more about the [topic] you just mentioned"), demonstrating contextual awareness across the conversation.

**Dependencies:**
* T21.G3.07: Build a multi-turn conversation with connected prompts

---

### Temperature Parameter Experimentation

**ID:** T21.G3.09
**Topic:** T21 – Chatbots & Prompting
**Skill:** Understand what the temperature parameter controls
**Description:** Students learn that temperature (ranging from 0 to 1) controls how creative or predictable ChatGPT's responses are: low temperature (0.1-0.3) gives consistent, focused answers; medium (0.5-0.7) balances creativity and consistency; high (0.8-1.0) produces more varied, creative responses. Assessment: Student explains in their own words what temperature does, provides examples of when to use low vs. high temperature (e.g., "Low for math facts, high for story ideas"), and correctly matches 4 out of 5 task scenarios to appropriate temperature settings.

**Dependencies:**
* T21.G3.01: Use the basic ChatGPT request block with default settings
* T21.G2.09: Change one element of a prompt to test its effect

---

**ID:** T21.G3.10
**Topic:** T21 – Chatbots & Prompting
**Skill:** Experiment with different temperature settings
**Description:** Students conduct experiments using the same prompt with different temperature values to observe how responses change. They test at least three different temperature settings (e.g., 0.2, 0.7, 1.0) and compare the results for creativity, variety, and appropriateness. Assessment: Student creates an experiment program that sends the same prompt to ChatGPT three times with different temperature settings (low, medium, high), documents the three responses, and explains which temperature worked best for their specific purpose.

**Dependencies:**
* T21.G3.09: Understand what the temperature parameter controls

---

### Response Length Management

**ID:** T21.G3.11
**Topic:** T21 – Chatbots & Prompting
**Skill:** Adjust the maximum length parameter for responses
**Description:** Students learn to use the length parameter to control approximately how long ChatGPT's responses will be. They understand that different tasks need different response lengths (short for quick facts, longer for detailed explanations) and practice setting appropriate lengths. Assessment: Student creates a program with three ChatGPT requests for different purposes, sets different appropriate length values for each (e.g., 50 for a quick definition, 200 for a detailed explanation, 500 for a story), and explains their length choices.

**Dependencies:**
* T21.G3.01: Use the basic ChatGPT request block with default settings
* T21.G1.09: Predict what type of answer a question will receive

---

**ID:** T21.G3.12
**Topic:** T21 – Chatbots & Prompting
**Skill:** Combine length limits with prompt instructions
**Description:** Students learn to use both the length parameter and explicit length instructions in their prompts (e.g., "Explain in 2 sentences" + length=100) for better control over response size. They understand that combining both methods produces more reliable results. Assessment: Student creates a program with three requests, each using both a length parameter setting and explicit length instructions in the prompt (e.g., "in one paragraph," "in 3 bullet points"), and demonstrates that responses match the intended length.

**Dependencies:**
* T21.G3.11: Adjust the maximum length parameter for responses
* T21.G2.02: Create simple prompts using Role-Task-Format structure

---

### AI Image Generation Basics

**ID:** T21.G3.13
**Topic:** T21 – Chatbots & Prompting
**Skill:** Generate images using DALL-E with descriptive prompts
**Description:** Students learn to use `OpenAI DALL-E: generate image with request [DESCRIPTION] resolution [512x512]` to create images from text descriptions. They understand that detailed, specific descriptions produce better images and practice writing clear image prompts. Assessment: Student creates a program that generates at least three different images using DALL-E, with each image prompt including at least four descriptive details (e.g., subject, colors, setting, style), and successfully produces images that match their descriptions.

**Dependencies:**
* T21.G2.02: Create simple prompts using Role-Task-Format structure
* T21.G2.11: Use picture-based organizers to design complex prompts

---

**ID:** T21.G3.14
**Topic:** T21 – Chatbots & Prompting
**Skill:** Choose appropriate image resolution for different purposes
**Description:** Students learn the three resolution options (256x256, 512x512, 1024x1024) and understand when to use each: smaller for icons or quick previews, medium for general use, larger for detailed images or printing. They practice selecting appropriate resolutions based on their project needs. Assessment: Student creates a program that generates the same image at all three resolutions, compares the results, and explains which resolution they would use for three different scenarios (e.g., "profile icon," "story illustration," "poster background").

**Dependencies:**
* T21.G3.13: Generate images using DALL-E with descriptive prompts

---

### Debugging ChatGPT Programs

**ID:** T21.G3.15
**Topic:** T21 – Chatbots & Prompting
**Skill:** Debug common ChatGPT block errors
**Description:** Students learn to identify and fix common problems in ChatGPT programs: incorrect variable names, missing prompts, wrong session types, inappropriate parameter values. They use systematic debugging strategies (check each parameter, test with simpler prompts, verify variable usage). Assessment: Student is given a program with at least three intentional errors (e.g., empty prompt, undefined variable, session type mismatch), successfully identifies all errors, fixes them, and explains what was wrong and how they fixed it.

**Dependencies:**
* T21.G3.01: Use the basic ChatGPT request block with default settings
* T21.G3.02: Store and display ChatGPT responses using variables
* T21.G3.05: Manage conversations using session types (new chat vs. continue)

---
