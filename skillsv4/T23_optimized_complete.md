ID: T23.GK.01
Topic: T23 – AI Perception
Skill: Match pictures of sensing
Description: Students drag friendly icons (eye, ear, hand) onto photos showing someone looking, listening, or pressing a big button, building the idea that helpers need different kinds of sensing. All activities use pictures and physical objects—no screens or blocks.



ID: T23.GK.02
Topic: T23 – AI Perception
Skill: Point to where a device "looks" or "listens"
Description: Students tap the camera spot on a tablet and the speaker/mic area on a toy or smart speaker, connecting device parts to senses. They use picture cards and physical devices—no code or programming environment.

Dependencies:
* T23.GK.01: Match pictures of sensing


ID: T23.GK.03
Topic: T23 – AI Perception
Skill: Choose when to uncover or quiet a helper
Description: In illustrated scenarios (covering a camera with a sticker, talking over loud music), students choose the action that lets the helper sense again (remove the sticker, make it quieter). Uses picture-based decision cards only.

Dependencies:
* T23.GK.02: Point to where a device "looks" or "listens"


---

## GRADE 1 SKILLS

ID: T23.G1.01
Topic: T23 – AI Perception
Skill: Find sensors on everyday devices
Description: Students look at pictures of a tablet, camera toy, smart speaker, and game controller and circle where the camera, microphone, and buttons are. They sort devices by what senses they use. Picture-based activity only.

Dependencies:
* T01.GK.03: Find the first and last pictures
* T23.GK.02: Point to where a device "looks" or "listens"


ID: T23.G1.02
Topic: T23 – AI Perception
Skill: Match sensors to human senses
Description: Students drag picture icons for "see," "hear," and "touch" to the matching device sensors (camera, mic, touchpad) to show the parallel. They identify which sensors help a robot "see" or "hear." Picture-based matching only.

Dependencies:
* T03.GK.02: Match parts to whole objects
* T23.GK.01: Match pictures of sensing


ID: T23.G1.03
Topic: T23 – AI Perception
Skill: Choose what a sensor can notice
Description: Given picture cards (light/dark room, loud music, soft pillow), students pick which things a camera, microphone, or touchpad can notice and which it cannot (e.g., a microphone can't see color). Picture-sorting activity.

Dependencies:
* T01.GK.04: Pick the pictures that make sense
* T23.G1.01: Find sensors on everyday devices


---

## GRADE 2 SKILLS

ID: T23.G2.01
Topic: T23 – AI Perception
Skill: Pick the right sensor for a job
Description: Students read short picture stories (e.g., "turn on light when someone claps," "open door when tag is tapped") and circle whether to use camera, microphone, or touch sensor to solve each task. Scenario-based decisions using illustrated cards.

Dependencies:
* T23.G1.03: Choose what a sensor can notice


ID: T23.G2.02
Topic: T23 – AI Perception
Skill: Spot when sensor data might be unclear
Description: Students compare pairs of pictures (bright vs dark room for a camera, quiet vs noisy room for a mic) and pick which one makes it harder for the sensor to understand. They explain why using simple words.

Dependencies:
* T23.G2.01: Pick the right sensor for a job


ID: T23.G2.03
Topic: T23 – AI Perception
Skill: Notice that devices sometimes "guess"
Description: Students compare two illustrated scenarios: one where a toy reacts to a button press; another where an app tries to recognize an animal sound. They identify which one is "guessing" from sensor input versus following a direct command.

Dependencies:
* T01.G1.01: Put pictures in order to plant a seed


---

## GRADE 3 SKILLS

ID: T23.G3.01
Topic: T23 – AI Perception
Skill: Describe a picture as a grid of tiny colors
Description: Students view a photo and its pixelated grid side by side in CreatiCode and explain that cameras store pictures as small colored squares (pixels). They use a simple sprite costume editor to highlight individual pixels and observe how changing brightness affects pixel colors.

Dependencies:
* T07.G3.01: Use a counted repeat loop
* T23.G2.01: Pick the right sensor for a job


ID: T23.G3.02
Topic: T23 – AI Perception
Skill: Describe sound as a wavy line of loud/soft
Description: Students see a simple waveform visualization for a clap vs a whisper and match which wave is which. They note that microphones turn sound into a line that goes up (louder) and down (softer). They may use a costume or backdrop showing waveforms.

Dependencies:
* T06.G3.05: Decide which event type to use for a behavior


ID: T23.G3.03
Topic: T23 – AI Perception
Skill: Tell whether a behavior uses sensing and guessing
Description: Students read simple program descriptions (e.g., "game starts when you press space" vs "door opens when it sees your face") and decide which ones require the device to sense and guess vs ones that follow a fixed button rule. They identify the event blocks that would be used.

Dependencies:
* T23.G3.02: Describe sound as a wavy line of loud/soft
* T09.G3.01.04: Display variable value on stage using the variable monitor


---

## GRADE 4 SKILLS

ID: T23.G4.01
Topic: T23 – AI Perception
Skill: Trace how lighting changes pixel data
Description: Students use a provided slider UI (built with basic blocks) to dim/brighten a sample image costume and observe which pixel areas get darker/brighter in the costume editor. They answer questions about why dark rooms make images harder for AI to read.

Dependencies:
* T06.G3.01: Build a green‑flag script that runs a 3–5 block sequence
* T07.G2.01: Identify when to use "repeat" vs "do once"
* T08.G3.05: Fix a condition that uses the wrong comparison operator
* T23.G2.02: Spot when sensor data might be unclear
* T23.G3.01: Describe a picture as a grid of tiny colors


ID: T23.G4.02
Topic: T23 – AI Perception
Skill: Choose a good setup for mic or camera
Description: Students examine 3 illustrated scenarios (e.g., backlit vs front-lit for camera, mic close vs far for microphone) and pick the best setup for clear input. They build a simple Scratch script that displays "good setup" or "needs improvement" messages.

Dependencies:
* T06.G3.01: Build a green‑flag script that runs a 3–5 block sequence
* T07.G2.01: Identify when to use "repeat" vs "do once"
* T08.G3.05: Fix a condition that uses the wrong comparison operator
* T23.G3.01: Describe a picture as a grid of tiny colors
* T23.G3.02: Describe sound as a wavy line of loud/soft


ID: T23.G4.03
Topic: T23 – AI Perception
Skill: Identify noise and simple fixes
Description: Students examine examples of blurry images, shaky video clips, or choppy audio recordings and select a simple fix (steady the device, add light, move to quieter spot) before any AI coding happens. They create a troubleshooting flowchart using sprites.

Dependencies:
* T01.G2.01: Find actions that repeat in everyday tasks
* T04.G2.03: Compare a long explicit description vs a compressed "repeat" description
* T06.G3.01: Build a green‑flag script that runs a 3–5 block sequence
* T07.G2.01: Identify when to use "repeat" vs "do once"
* T08.G3.05: Fix a condition that uses the wrong comparison operator
* T23.G3.01: Describe a picture as a grid of tiny colors


---

## GRADE 5 SKILLS

ID: T23.G5.01
Topic: T23 – AI Perception
Skill: Compare what people see vs what pixels show
Description: Students look at a clear photo and its coarse pixel version side by side and explain what detail is lost for the computer but obvious to a person (e.g., small text, faint objects). They use the costume editor to zoom in and count pixels.

Dependencies:
* T08.G3.05: Fix a condition that uses the wrong operator
* T23.G4.01: Trace how lighting changes pixel data
* T06.G3.01: Build a green‑flag script that runs a 3–5 block sequence
* T09.G3.01.01: Create a new variable with a descriptive name


ID: T23.G5.02
Topic: T23 – AI Perception
Skill: Explain why an AI might mis-hear or mis-see
Description: Given examples of mis-recognized words or images (accent, shadowed face), students identify likely causes (background noise, low light, unusual angle) and suggest one fix (move closer, add light, speak clearly). They build a simple diagnostic tool.

Dependencies:
* T08.G3.05: Fix a condition that uses the wrong operator
* T23.G4.03: Identify noise and simple fixes
* T23.G3.03: Tell whether a behavior uses sensing and guessing
* T06.G3.01: Build a green‑flag script that runs a 3–5 block sequence
* T09.G3.01.01: Create a new variable with a descriptive name


ID: T23.G5.03
Topic: T23 – AI Perception
Skill: Choose safe ways to handle sensor data
Description: Students compare actions for camera/mic data (e.g., "keep photos only on device" vs "share raw recordings with strangers") and classify them as safe or risky. They link perception to privacy before coding actual AI blocks.

Dependencies:
* T08.G3.05: Fix a condition that uses the wrong operator
* T23.G4.02: Choose a good setup for mic or camera
* T23.G3.03: Tell whether a behavior uses sensing and guessing
* T06.G3.01: Build a green‑flag script that runs a 3–5 block sequence
* T09.G3.01.01: Create a new variable with a descriptive name


ID: T23.G5.04
Topic: T23 – AI Perception
Skill: Identify when AI sensing might be unfair
Description: Students examine scenarios where AI perception might work poorly for some groups (face recognition in poor lighting, voice recognition with different accents) and suggest basic fairness improvements (better lighting, multiple language options).

Dependencies:
* T08.G3.05
* T23.G4.03
* T23.G3.03
* T09.G3.03
* T06.G3.01: Build a green‑flag script that runs a 3–5 block sequence


ID: T23.G5.05.01
Topic: T23 – AI Perception
Skill: Identify what data different detection types provide
Description: Students learn that AI vision blocks detect specific features with distinct outputs: hand detection (finger positions, curl angles, direction), body detection (body part positions), and face detection (face locations, landmarks). They match detection types to their data outputs using picture cards showing tables with x/y coordinates, angles, and other values.

Dependencies:
* T10.G5.04
* T23.G5.01
* T09.G3.03
* T06.G3.01: Build a green‑flag script that runs a 3–5 block sequence


ID: T23.G5.05.02
Topic: T23 – AI Perception
Skill: Map detection data to table structures
Description: Students examine annotated examples showing how each detection type stores data in tables: hand detection (47 rows per hand with sections for finger summaries, 2D landmarks, 3D landmarks), body detection (17 keypoints + 4 limbs), face detection (13 rows per face with tilt angle and 6 landmark positions). They practice reading table diagrams and identifying which row/column contains specific information (e.g., "Which row has index finger curl?").

Dependencies:
* T10.G5.04
* T23.G5.05.01: Identify what data different detection types provide
* T09.G3.03


ID: T23.G5.05.03
Topic: T23 – AI Perception
Skill: Understand perception API workflow patterns
Description: Students learn the common pattern for perception APIs: (1) start detection with configuration, (2) read results from output table, (3) process data with conditionals, (4) stop detection. They match API blocks to workflow steps (start→read→process→stop) using diagrams. Picture-based workflow analysis, no coding yet.

Dependencies:
* T23.G5.05.02: Map detection data to table structures
* T06.G3.01: Build a green‑flag script that runs a 3–5 block sequence
* T09.G3.01.01: Create a new variable with a descriptive name


---

## GRADE 6 SKILLS

ID: T23.G6.01.01
Topic: T23 – AI Perception
Skill: Capture a single spoken phrase with basic speech recognition
Description: Students use the basic speech recognition flow: `start recognizing speech in [English (United States) v] record as []` (with default language), wait briefly, then `end speech recognition` to capture a single spoken word or phrase. The recognized text is stored in a variable (not in a table). They display the result using the `text from speech` reporter block and a `say` block or variable monitor. Common issues include silent rooms (no input detected), background noise (mis-recognition), and recognition delay (typically 1-3 seconds after speaking stops). They learn the workflow: start detection → speak → wait for processing → end detection → read result. They understand that the system listens continuously while detection is active and that ending detection triggers the final transcription. They implement basic error handling for empty results (no speech detected).

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G5.02: Explain why an AI might mis-hear or mis-see


ID: T23.G6.01.02
Topic: T23 – AI Perception
Skill: Select speech recognition language and observe accuracy differences
Description: Students extend basic speech recognition by exploring the language dropdown in `start recognizing speech in [LANGUAGE v] record as []`. They test recognition with different languages (English, Spanish, Chinese, etc.) and observe how selecting the correct language improves accuracy. They build a simple app that lets users choose their language before speaking.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G6.01.01: Capture a single spoken phrase with basic speech recognition


ID: T23.G6.01.03
Topic: T23 – AI Perception
Skill: Use continuous speech recognition for real-time transcription
Description: Students learn continuous speech recognition: `start continuous speech recognition in [LANGUAGE v] into list [listname v]` to begin streaming recognition. The list continuously updates with recognized phrases. They use `stop continuous speech recognition` to end. They build a live transcript display that updates as the user speaks.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G6.01.02: Select speech recognition language and observe accuracy differences


ID: T23.G6.01.04
Topic: T23 – AI Perception
Skill: Handle speech recognition errors and implement retry logic
Description: Students implement error handling for speech recognition failures: check if result is empty (no speech detected), provide visual/audio feedback when recognition fails, implement retry mechanism (allow 3 attempts), and offer alternative input methods (text entry, button selection) when speech consistently fails. They learn to detect timeout scenarios and provide helpful error messages to users.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G6.01.03: Use continuous speech recognition for real-time transcription


ID: T23.G6.02.01
Topic: T23 – AI Perception
Skill: Convert text to speech with basic settings
Description: Students use the `say [TEXT] in [LANGUAGE v] as [VOICETYPE v] speed (SPEEDRATIO) pitch (PITCHRATIO) volume (VOLUMERATIO) store sound as []` block to convert text to speech. They experiment with different languages, voice types (Male/Female), and adjust speed/pitch/volume parameters (default 100, range 50-200) to create different speaking styles.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G5.02: Explain why an AI might mis-hear or mis-see


ID: T23.G6.02.02
Topic: T23 – AI Perception
Skill: Control TTS playback using the stop speaking block
Description: Students learn to interrupt text-to-speech output using the `stop speaking` block. They implement scenarios where TTS needs to be cancelled: user clicks skip button, new urgent message arrives, or timeout occurs. They manage the timing of TTS to prevent overlapping speech and implement queuing systems for multiple TTS messages.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G6.02.01: Convert text to speech with basic settings


ID: T23.G6.02.03
Topic: T23 – AI Perception
Skill: Save and reuse text-to-speech audio recordings
Description: Students use the `store sound as []` parameter in the TTS block to save generated speech as a sound file that can be replayed without regenerating. They learn when to pre-generate audio (static messages, frequently used phrases) vs generate on-demand (dynamic content). They implement a sound library system that caches commonly used TTS outputs for faster playback and reduced API calls.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G6.02.02: Control TTS playback using the stop speaking block


ID: T23.G6.03.01
Topic: T23 – AI Perception
Skill: Build a two-way voice chatbot loop
Description: Students combine speech-to-text (`start recognizing speech in [LANGUAGE v] record as []` → `end speech recognition` → `text from speech`), ChatGPT request block (`OpenAI ChatGPT: request … result [variable]`), and text-to-speech (`say [TEXT] in [LANGUAGE v] as [VOICETYPE v] …`) to build a voice assistant. They implement turn-taking: listen → process → speak → repeat. They learn the complete conversational flow: detect when user stops speaking, send transcript to ChatGPT API, receive response text, convert response to speech, play audio output, then restart listening. They handle timing issues like waiting for TTS to complete before listening again and managing conversation state across turns. Note: Requires T22 ChatGPT knowledge.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T22.G6.01: Trace how a chatbot script processes each turn
* T23.G6.01.02: Select speech recognition language and observe accuracy differences
* T23.G6.02.01: Convert text to speech with basic settings


ID: T23.G6.03.02
Topic: T23 – AI Perception
Skill: Use OpenAI Whisper for advanced speech transcription
Description: Students use `OpenAI: start recognizing speech in [LANGUAGE v] record as []` → `end speech recognition` → `text from speech` for high-accuracy speech recognition via OpenAI Whisper API. They compare Whisper's performance with basic speech recognition, especially in noisy environments or with accents, and learn trade-offs (accuracy vs. speed, API costs).

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T22.G6.01: Trace how a chatbot script processes each turn
* T23.G6.01.02: Select speech recognition language and observe accuracy differences


ID: T23.G6.04.01
Topic: T23 – AI Perception
Skill: Set up hand detection and view debug output
Description: Students use `run hand detection table [TABLENAME v] debug [yes v] show video [yes v]` to turn on the front camera and detect hands. They explore the debug mode (draws keypoints on video) and show/hide video options. They observe how the detection responds to hand movements.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G5.05.03: Understand perception API workflow patterns


ID: T23.G6.04.02.01
Topic: T23 – AI Perception
Skill: Understand hand detection table structure
Description: Students learn the hand detection table structure: 47 rows per detected hand organized into three sections: (1) rows 1-5 contain finger summaries (thumb, index, middle, ring, pinky) with columns [hand, part, curl, dir, x, y, z], (2) rows 6-26 contain 2D landmark positions, (3) rows 27-47 contain 3D landmark positions. They identify which row contains specific finger data and understand that curl ranges from 0° (fully closed/fist) to 180° (fully extended/straight), direction ranges from 0° to 360° indicating pointing direction, and x/y are screen coordinates while z is depth. They practice locating specific data: "Which row has index finger curl?" (row 2).

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G6.04.01: Set up hand detection and view debug output


ID: T23.G6.04.02.02
Topic: T23 – AI Perception
Skill: Read finger curl values from hand detection table
Description: Students read curl values from the hand detection table (rows 1-5) to get finger curl angles. Each row contains: hand ID (which hand: 0=right, 1=left), part name (finger name), curl angle (0-180°), direction angle (0-360°), and x/y/z coordinates. They use table read blocks to extract curl values for specific fingers and understand that curl measures how bent the finger is: 0° = closed fist, 180° = straight finger.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T10.G5.04: Read a cell value from a table
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G6.04.02.01: Understand hand detection table structure


ID: T23.G6.04.02.03
Topic: T23 – AI Perception
Skill: Display hand detection data using variable monitors
Description: Students display finger curl values on screen using variable monitors or say blocks. They create a display showing all five finger curl angles updating in real-time as the hand moves. They implement basic gesture detection by checking curl thresholds: pointing (index curl > 170, others < 170) or fist (all curl < 90). No advanced UI integration yet, just displaying values and simple threshold-based detection.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T10.G5.04: Read a cell value from a table
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G6.04.02.02: Read finger curl values from hand detection table


ID: T23.G6.04.03
Topic: T23 – AI Perception
Skill: Read finger direction data for advanced gesture recognition
Description: Students extend hand detection by reading the direction (dir) column from the hand detection table. Each finger has a direction indicating which way it's pointing (up, down, left, right). They combine curl and direction to recognize complex gestures: "thumbs up" = thumb extended (curl > 170) + pointing up, "peace sign" = index and middle extended + pointing up.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T10.G5.04: Read a cell value from a table
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G6.04.02.03: Display hand detection data using variable monitors


ID: T23.G6.04.04
Topic: T23 – AI Perception
Skill: Recognize basic gestures from hand detection data
Description: Students use hand detection curl and direction data to identify 3-5 basic gestures: fist (all fingers curl < 90), open hand (all curl > 150), pointing (index curl > 170, others < 90), thumbs up (thumb curl > 170 and dir near 0°), peace sign (index and middle curl > 170, others < 90). They use if-blocks to check conditions and display gesture names. No UI integration yet, just gesture recognition logic.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T09.G5.01: Use multiple variables together in a single expression
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G6.04.03: Read finger direction data for advanced gesture recognition


ID: T23.G6.04.05
Topic: T23 – AI Perception
Skill: Drive UI elements with live hand detection
Description: Students read x/y coordinates from the hand detection table (wrist or index finger position) and convert them into UI widget interactions: move a pointer sprite, adjust a slider, trigger hover states. They learn to hide the camera feed (`show video [no v]`) to reduce distraction while keeping detection active.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Use multiple variables together in a single expression
* T10.G5.04: Read a cell value from a table
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G6.04.04: Recognize basic gestures from hand detection data


ID: T23.G6.04.06
Topic: T23 – AI Perception
Skill: Detect and differentiate between left and right hands
Description: Students read the hand ID from the hand detection table (column: hand, value: 0=right hand, 1=left hand) to determine which hand is detected. They implement applications that require specific hand usage: "raise right hand to answer," "use left hand for menu," or two-handed gestures that coordinate both hands. They handle scenarios where both hands are visible and track each hand independently.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Use multiple variables together in a single expression
* T10.G5.04: Read a cell value from a table
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G6.04.04: Recognize basic gestures from hand detection data


ID: T23.G6.04.07
Topic: T23 – AI Perception
Skill: Track multiple hands simultaneously
Description: Students process hand detection data when multiple hands are visible. The table contains 47 rows per hand, so 2 hands = 94 rows. They iterate through the table to separate data for each hand (rows 1-47 = first hand, rows 48-94 = second hand), track gestures for each hand independently, and implement two-handed interactions: clapping detection (both hands close together), measuring hand distance, or cooperative gestures requiring both hands.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Use multiple variables together in a single expression
* T10.G5.04: Read a cell value from a table
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G6.04.06: Detect and differentiate between left and right hands


ID: T23.G6.04.08
Topic: T23 – AI Perception
Skill: Stop hand detection when no longer needed
Description: Students implement proper cleanup for hand detection by stopping the detection when it's no longer needed. They understand that detection consumes resources (camera, processing) and should be stopped when: switching to different input mode, pausing the application, or when detection task is complete. They use a stop block or proper event handling to end detection gracefully and release the camera. They implement detection lifecycle: start → use → stop, preventing resource leaks.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Use multiple variables together in a single expression
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G6.04.02.03: Display hand detection data using variable monitors


ID: T23.G6.06.01
Topic: T23 – AI Perception
Skill: Apply moving average to smooth noisy sensor data
Description: Students implement moving average smoothing: store the last 5 wrist position readings in a list, calculate the average of these values, and use the averaged position to move a sprite. They observe how averaging reduces jittery movement and understand the trade-off between smoothness (larger window) and responsiveness (smaller window). They learn when to apply smoothing (continuous tracking) vs when not to (detecting quick gestures).

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Use multiple variables together in a single expression
* T09.G5.05: Use the accumulator pattern to compute running totals
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G4.03: Identify noise and simple fixes
* T23.G6.04.02.03: Display hand detection data using variable monitors


ID: T23.G6.06.02
Topic: T23 – AI Perception
Skill: Use clamping to limit sensor values to valid ranges
Description: Students implement value clamping to constrain sensor readings to valid ranges. They use conditional blocks to check if a value exceeds boundaries and reset it to the boundary value: `if position < 0 then set position to 0`, `if position > 480 then set position to 480`. They apply clamping to prevent sprites from moving off-screen, keep angles within 0-360 range, and filter out impossible sensor values that indicate errors.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G4.03: Identify noise and simple fixes
* T23.G6.04.02.03: Display hand detection data using variable monitors


ID: T23.G6.06.03
Topic: T23 – AI Perception
Skill: Implement debouncing to filter rapid fluctuations
Description: Students implement debouncing to ignore rapid changes in sensor data. They require a value to remain stable for a minimum time (e.g., 0.5 seconds) before accepting it as valid. For gesture detection, they check that a gesture is maintained for multiple consecutive frames (3+ frames) before triggering an action. This prevents false positives from brief sensor noise or accidental hand movements. They understand the trade-off between reliability and responsiveness.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G4.03: Identify noise and simple fixes
* T23.G6.04.02.03: Display hand detection data using variable monitors


ID: T23.G6.06.04
Topic: T23 – AI Perception
Skill: Create watchdog timers to detect and recover from sensor dropouts
Description: Students implement watchdog timers to detect when sensors stop providing data. They track the time since last valid sensor reading and trigger recovery actions if too much time passes (e.g., 2 seconds with no hand detected). Recovery actions include: displaying "hand not detected" message, switching to alternative input mode, or restarting the detection system. They handle scenarios where hands temporarily leave the camera frame and distinguish between brief dropouts (ignore) and extended absence (notify user).

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G4.03: Identify noise and simple fixes
* T23.G6.04.02.03: Display hand detection data using variable monitors


ID: T23.G6.07
Topic: T23 – AI Perception
Skill: Choose continuous vs. event-driven detection patterns
Description: Students compare two detection patterns: (1) continuous polling in forever loop (constantly read table and update), (2) event-driven (start detection, wait for specific condition, then act). They implement both patterns with hand detection: continuous mode moves sprite smoothly following hand, event-driven mode triggers action when gesture detected. They discuss trade-offs: continuous is smooth but CPU-intensive, event-driven is efficient but may miss quick gestures.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T09.G5.01: Use multiple variables together in a single expression
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G6.04.04: Recognize basic gestures from hand detection data
* T23.G6.06.01: Apply moving average to smooth noisy sensor data


ID: T23.G6.08
Topic: T23 – AI Perception
Skill: Add consent and privacy controls for sensor use
Description: Students add clear permission requests before enabling camera/mic detection ("This app needs your camera. Allow?"), provide easy on/off toggle buttons, and implement data retention limits (clear table after use). They explain to users what data is collected and why, using T16 labels and dialogs.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T16.G6.01: Attach a button to a sprite and respond to clicks
* T23.G5.03: Choose safe ways to handle sensor data


ID: T23.G6.09.01.01
Topic: T23 – AI Perception
Skill: Set up 2D body detection and view debug output
Description: Students use `run 2D body part recognition single person [yes v] table [TABLENAME v] debug [yes v]` to detect body landmarks. They explore debug mode (draws skeleton on video) and understand single-person vs multi-person mode. They observe how the detection responds to body movements and poses.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T10.G5.04: Read a cell value from a table
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G5.05.03: Understand perception API workflow patterns


ID: T23.G6.09.01.02
Topic: T23 – AI Perception
Skill: Understand body detection table structure
Description: Students learn the body detection table structure with 21 rows per person: 17 keypoint rows (nose, left_eye, right_eye, left_ear, right_ear, left_shoulder, right_shoulder, left_elbow, right_elbow, left_wrist, right_wrist, left_hip, right_hip, left_knee, right_knee, left_ankle, right_ankle) plus 4 limb measurements (left_arm, right_arm, left_leg, right_leg). Table columns are: id, part, x, y, curl, dir. They understand that keypoints can be unreliable when occluded (hidden) and that confidence affects detection quality. They practice identifying which row contains specific body parts.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T10.G5.04: Read a cell value from a table
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G6.09.01.01: Set up 2D body detection and view debug output


ID: T23.G6.09.01.03
Topic: T23 – AI Perception
Skill: Read body keypoint positions from the table
Description: Students read body keypoint x/y coordinates from the body detection table. They extract specific keypoint positions (e.g., wrist, shoulder, knee) and display them using variable monitors or by moving sprites to keypoint locations. They implement basic pose visualization by drawing lines between connected keypoints (shoulder to elbow, elbow to wrist, etc.) to create a stick-figure representation.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T10.G5.04: Read a cell value from a table
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G6.09.01.02: Understand body detection table structure


ID: T23.G6.09.01.04
Topic: T23 – AI Perception
Skill: Stop body detection when no longer needed
Description: Students implement proper cleanup for body detection by stopping the detection when it's no longer needed using the stop block. They understand that detection consumes resources and should be stopped when: switching tasks, pausing the application, or when detection is complete. They implement detection lifecycle: start → use → stop, preventing resource leaks and allowing camera use by other features.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G6.09.01.03: Read body keypoint positions from the table


ID: T23.G6.09.02
Topic: T23 – AI Perception
Skill: Detect body poses and trigger actions
Description: Students calculate angles between body landmarks (e.g., arm angle from shoulder-elbow-wrist positions) to detect specific poses. They trigger actions when poses are detected: "arms up" = both wrists above shoulders, "squat" = knees below hips. They use if-blocks to compare y-coordinates and implement pose-triggered events.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T10.G5.04: Read a cell value from a table
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G6.09.01.04: Stop body detection when no longer needed


ID: T23.G6.09.03
Topic: T23 – AI Perception
Skill: Use 3D pose detection for depth-aware body tracking
Description: Students use `run 3D pose detection debug [yes v] table [TABLENAME v]` to detect body landmarks with depth information (x, y, z coordinates). They compare 2D vs 3D pose detection, understanding that 3D provides distance from camera. They visualize the z-coordinate to understand depth perception and build applications that measure 3D movements (e.g., squat depth, forward reach).

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T10.G5.04: Read a cell value from a table
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G6.09.02: Detect body poses and trigger actions


ID: T23.G6.10.01
Topic: T23 – AI Perception
Skill: Set up face detection and view detected faces
Description: Students use `run face detection debug [yes v] and write into table [TABLENAME v]` to turn on the front camera and detect faces. They observe the debug mode (draws bounding boxes around faces) and explore the result table structure, which contains face positions and facial landmarks.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T10.G5.04: Read a cell value from a table
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G5.05.03: Understand perception API workflow patterns


ID: T23.G6.10.02.01
Topic: T23 – AI Perception
Skill: Understand face detection table structure
Description: Students learn the face detection table structure with 13 rows per detected face: 1 row for tilt angle, plus 12 rows for 6 facial landmark positions (left_eye, right_eye, nose, mouth, left_ear, right_ear, each with x and y coordinates). Table columns are: ID, variable, value. They practice parsing the table: read ID column to differentiate between multiple faces, read variable column to identify which landmark, and read value column for the coordinate. They understand how lighting affects detection accuracy.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T10.G5.04: Read a cell value from a table
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G6.10.01: Set up face detection and view detected faces


ID: T23.G6.10.02.02
Topic: T23 – AI Perception
Skill: Read face position and tilt angle from table
Description: Students read face tilt angle and landmark positions from the face detection table. They extract face center coordinates (average of eye positions) and tilt angle to understand face orientation. They display these values using variable monitors and understand that tilt angle indicates head rotation.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T10.G5.04: Read a cell value from a table
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G6.10.02.01: Understand face detection table structure


ID: T23.G6.10.02.03
Topic: T23 – AI Perception
Skill: Move a sprite to follow detected face
Description: Students implement face-following behavior by reading face center coordinates from the face detection table and moving sprites to match. They handle edge cases like multiple faces detected simultaneously (choose first face) and faces partially out of frame (clamp to screen bounds). They implement error handling for "no face detected" scenarios. They note that face data can be noisy and may need smoothing for smooth sprite movement.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T10.G5.04: Read a cell value from a table
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G6.10.02.02: Read face position and tilt angle from table


ID: T23.G6.11
Topic: T23 – AI Perception
Skill: Use NLP sentence analysis to extract parts of speech
Description: Students use `analyze sentence [SENTENCE] and write into table [TABLENAME v]` to analyze sentence structure and extract parts of speech (nouns, verbs, adjectives, etc.) from recognized speech or text input. They implement applications that parse voice commands to identify action words (verbs) and objects (nouns): "move the robot forward" → action: move, object: robot, direction: forward. They build more flexible command recognition that handles variations in phrasing ("go forward" vs "move ahead" vs "drive forward").

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G6.01.02: Select speech recognition language and observe accuracy differences


ID: T23.G6.12
Topic: T23 – AI Perception
Skill: Compare Azure vs OpenAI Whisper speech recognition performance
Description: Students run comparative tests between the default speech recognition (Azure) and OpenAI Whisper API. They test both systems with the same audio samples in different conditions: clear speech, accented speech, noisy environment, technical vocabulary, and multiple languages. They document accuracy differences, latency (response time), cost implications, and reliability. They create a decision matrix for choosing the appropriate speech recognition engine based on application requirements.

Dependencies:
* T05.G5.01: Write clear user needs and requirements for a small app
* T06.G5.01: Identify standard event patterns in a small game
* T08.G5.01: Use a simple if in a script
* T09.G5.01: Display variable value on stage using the variable monitor
* T09.G5.01: Use multiple variables together in a single expression
* T11.G5.01: Decompose a problem into logical custom block boundaries
* T15.G5.01: Coordinate scene changes with broadcasts
* T23.G6.03.02: Use OpenAI Whisper for advanced speech transcription


---

## GRADE 7 SKILLS

ID: T23.G7.00
Topic: T23 – AI Perception
Skill: Choose appropriate input modality for application context
Description: Students analyze application scenarios (noisy cafe, hands-free cooking, private space, public kiosk) and select the best input modality: voice-only, gesture-only, pose-only, or combinations. They consider accuracy (noisy environment reduces voice accuracy), user effort (hands-free favors voice/pose), privacy (voice reveals more than gesture), and accessibility. They create a decision matrix comparing modalities.

Dependencies:
* T23.G6.03.01: Build a two-way voice chatbot loop
* T23.G6.04.05: Drive UI elements with live hand detection
* T23.G6.09.02: Detect body poses and trigger actions
* T23.G5.02: Explain why an AI might mis-hear or mis-see


ID: T23.G7.01
Topic: T23 – AI Perception
Skill: Define a reusable gesture dictionary
Description: Students capture hand detection output (finger curl, dir, x/y positions) into a table, label each pattern ("thumbs up," "peace sign," "stop," "pointing"), and create custom reporter blocks that return the detected gesture name. They implement at least four gestures plus a "none detected" state, using T11 custom block patterns.

Dependencies:
* T10.G5.04: Read a cell value from a table
* T11.G5.02: Define a custom block with one parameter
* T23.G6.04.04: Recognize basic gestures from hand detection data
* T23.G6.04.05: Drive UI elements with live hand detection


ID: T23.G7.01.02
Topic: T23 – AI Perception
Skill: Combine inputs with simple OR logic
Description: Students build interactions where users can choose different input methods: "say 'next' OR perform swipe gesture" to advance, "press space bar OR raise hand" to start game. They use OR conditions to check multiple inputs and trigger the same action. They learn when OR logic is appropriate (giving users choices) vs. when specific input is required. Simpler than AND multimodal confirmation (G7.02).

Dependencies:
* T23.G7.01: Define a reusable gesture dictionary
* T23.G6.03.01: Build a two-way voice chatbot loop


ID: T23.G7.02
Topic: T23 – AI Perception
Skill: Require multimodal confirmation (voice + gesture)
Description: Students design safety-critical interactions (purchase confirmation, delete save file, launch simulation) that require matching voice command AND specific gesture to proceed. They manage sequence state (which input came first?), implement timeouts (confirmation expires after 5 seconds), and provide clear feedback on partial completion ("voice confirmed, waiting for gesture").

Dependencies:
* T09.G5.05: Use the accumulator pattern to compute running totals
* T23.G7.01: Define a reusable gesture dictionary
* T23.G6.03.01: Build a two-way voice chatbot loop
* T23.G6.04.05: Drive UI elements with live hand detection


ID: T23.G7.03
Topic: T23 – AI Perception
Skill: Score a pose-based challenge with coaching tips
Description: Students build a fitness or dance coaching app using `run 3D pose detection debug [yes v] table [result v]`. They detect whether player meets angle/position thresholds for a sequence (squat → jump → arms up), award points, display progress timeline, and show coaching text ("raise elbows higher," "squat deeper") based on which landmarks failed. They use z-coordinates to measure depth movements.

Dependencies:
* T23.G6.09.03: Use 3D pose detection for depth-aware body tracking
* T23.G6.06.01: Apply moving average to smooth noisy sensor data


ID: T23.G7.04
Topic: T23 – AI Perception
Skill: Monitor detection accuracy across different users
Description: Students design an accessibility log where each speech/gesture event is recorded with user metadata (age range, device type, lighting condition, language) plus outcome (success/failure). They calculate accuracy rates per group (success rate = correct detections / total attempts) and identify significant disparities (>20% difference between groups), such as low-light users having 40% success vs 90% in good light. They propose adjustments based on data.

Dependencies:
* T23.G6.06.01: Apply moving average to smooth noisy sensor data
* T23.G5.04: Identify when AI sensing might be unfair


ID: T23.G7.05
Topic: T23 – AI Perception
Skill: Implement fairness safeguards for perception systems
Description: Students implement measures to improve fairness: multiple attempts for failed recognition (3 tries before error), alternative input methods when sensors struggle (switch from voice to text input if speech fails), user feedback collection for system improvement, and adaptive thresholds that adjust to user patterns.

Dependencies:
* T23.G6.08: Add consent and privacy controls for sensor use
* T23.G5.04: Identify when AI sensing might be unfair


ID: T23.G7.06
Topic: T23 – AI Perception
Skill: Build a calibration wizard for sensors
Description: Students create a multi-step UI wizard (using T16 UI patterns) that guides users through sensor setup: microphone volume check (speak and see level), lighting test (show brightness meter), gesture framing (show silhouette guide). Each step runs a quick sensor test, displays current readings, and offers fixes ("move closer," "increase room light," "adjust camera angle").

Dependencies:
* T23.G6.06.01: Apply moving average to smooth noisy sensor data
* T23.G5.02: Explain why an AI might mis-hear or mis-see


ID: T23.G7.07
Topic: T23 – AI Perception
Skill: Optimize perception system performance
Description: Students identify and fix perception performance issues: reduce detection frame rate (process every 3rd frame instead of every frame), limit table size (clear old data), disable debug visualization in production, use efficient data structures (variables for single values instead of searching tables). They measure and compare performance before/after optimization using timer blocks. They understand trade-offs between accuracy and speed.

Dependencies:
* T23.G7.06: Build a calibration wizard for sensors
* T23.G6.07: Choose continuous vs. event-driven detection patterns


ID: T23.G7.08
Topic: T23 – AI Perception
Skill: Compare different AI detection algorithms
Description: Students compare different AI perception algorithms available in CreatiCode: hand detection vs body pose detection for gesture recognition, 2D vs 3D pose detection for movement tracking, Azure vs Whisper for speech recognition. They evaluate trade-offs: accuracy vs speed, resource usage vs reliability, cost vs performance. They document decision criteria and create guidelines for algorithm selection based on application requirements (real-time performance, accuracy needs, device capabilities).

Dependencies:
* T23.G6.09.03: Use 3D pose detection for depth-aware body tracking
* T23.G6.12: Compare Azure vs OpenAI Whisper speech recognition performance


ID: T23.G7.09
Topic: T23 – AI Perception
Skill: Build error recovery and fallback systems
Description: Students design robust perception systems that gracefully handle sensor failures. They implement fallback hierarchies: primary sensor fails → switch to backup sensor → if both fail → switch to manual input. They create error detection systems that identify sensor malfunctions (frozen data, impossible values, timeout), automatic recovery attempts (restart detection, recalibrate), and user notifications with actionable guidance. They test recovery systems by simulating failures.

Dependencies:
* T23.G6.06.04: Create watchdog timers to detect and recover from sensor dropouts
* T23.G7.01.02: Combine inputs with simple OR logic


---

## GRADE 8 SKILLS

ID: T23.G8.00
Topic: T23 – AI Perception
Skill: Understand supervised learning for perception classification
Description: Students learn the supervised learning workflow for gesture/pose classification: (1) collect labeled examples (record hand positions for "thumbs up," "peace sign," etc.), (2) train a classifier using the KNN blocks (`create KNN number classifier from table [training_data v] K [3] named [classifier1]`), (3) evaluate on test data. They understand that more training examples improve accuracy and that K value affects sensitivity to noise.

Dependencies:
* T08.G6.01: Use conditionals to control simulation steps
* T09.G6.01: Model real-world quantities using variables and formulas
* T23.G7.01: Define a reusable gesture dictionary
* T02.G6.01: Learn the pseudocode generation block
* T07.G6.01: Trace nested loops with variable bounds
* T10.G6.01: Sort a table by a column


ID: T23.G8.01
Topic: T23 – AI Perception
Skill: Offer interchangeable input modes with accessibility rules
Description: Students build a settings panel where users choose "voice only," "gesture only," or "hybrid" control mode. Each mode updates UI instructions, disables irrelevant widgets, and logs active mode for analytics. They implement auto-switching: if active sensor fails (e.g., hand leaves frame), automatically switch to voice mode and notify user.

Dependencies:
* T08.G6.01: Use conditionals to control simulation steps
* T09.G6.01: Model real-world quantities using variables and formulas
* T23.G7.02: Require multimodal confirmation (voice + gesture)
* T23.G6.03.01: Build a two-way voice chatbot loop
* T07.G6.01: Trace nested loops with variable bounds
* T09.G6.02: Apply operator precedence rules (PEMDAS) in expressions
* T10.G6.01: Sort a table by a column


ID: T23.G8.01.02
Topic: T23 – AI Perception
Skill: Practice KNN classification with simple numeric data
Description: Students practice KNN with a simple dataset before gesture classification: given a table of measurements (height, weight) and labels (category), they use `create KNN number classifier from table [training v] K [3] named [simple]` to train a classifier, then test it with new data using `predict for table [test v] with classifier [simple] show neighbors [yes v]`. They experiment with K values (1, 3, 5) and observe how it affects predictions. They understand KNN finds "similar" examples.

Dependencies:
* T23.G8.00: Understand supervised learning for perception classification
* T10.G6.02: Sort a table by a column
* T03.G6.01: Propose modules for a medium project
* T07.G6.01: Trace nested loops with variable bounds
* T10.G6.01: Sort a table by a column


ID: T23.G8.01.03
Topic: T23 – AI Perception
Skill: Split collected data into training and test sets
Description: Students learn the importance of separating data into training and test sets to evaluate classifier performance accurately. They implement data splitting: collect 100 samples, use 70 for training and 30 for testing (70/30 split). They understand that testing on training data gives falsely optimistic results and that test data must represent real-world usage. They implement random sampling to ensure balanced splits and avoid bias (equal representation of each gesture class in both sets).

Dependencies:
* T10.G6.02: Sort a table by a column
* T23.G8.01.02: Practice KNN classification with simple numeric data
* T07.G6.01: Trace nested loops with variable bounds
* T10.G6.01: Sort a table by a column
* T14.G6.01.01: Track game state with variable


ID: T23.G8.02.01
Topic: T23 – AI Perception
Skill: Create data collection UI for gesture samples
Description: Students build a data collection interface for training custom gesture classifiers. They create UI widgets (buttons for each gesture class, counter showing samples collected, visual feedback during recording) and implement the collection workflow: user selects gesture type → performs gesture → system captures hand detection data (curl, dir, x/y for all fingers) → stores in training table with label. They collect at least 20 samples per gesture class and implement quality checks (reject samples with no hand detected).

Dependencies:
* T16.G6.01: Attach a button to a sprite and respond to clicks
* T10.G6.02: Sort a table by a column
* T23.G7.01: Define a reusable gesture dictionary
* T03.G6.01: Propose modules for a medium project
* T07.G6.01: Trace nested loops with variable bounds
* T10.G6.01: Sort a table by a column


ID: T23.G8.02.02
Topic: T23 – AI Perception
Skill: Train KNN classifier with collected gesture data
Description: Students use collected gesture data to train a KNN classifier. They structure the training table correctly: each row is one sample, columns contain finger curl/dir values and x/y positions (features), final column contains gesture label (class). They use `create KNN number classifier from table [training_data v] K [3] named [gestureClassifier]` to create the classifier and experiment with different K values. They understand the training process: KNN stores all training examples and uses them for comparison during prediction.

Dependencies:
* T10.G6.02: Sort a table by a column
* T23.G8.00: Understand supervised learning for perception classification
* T23.G8.02.01: Create data collection UI for gesture samples
* T02.G6.01: Learn the pseudocode generation block
* T07.G6.01: Trace nested loops with variable bounds
* T08.G6.01a: Use conditionals in physics simulations


ID: T23.G8.02.03
Topic: T23 – AI Perception
Skill: Deploy trained classifier to recognize live gestures
Description: Students deploy their trained KNN classifier to recognize gestures in real-time. They implement the prediction workflow: capture live hand detection data → format as test table row → use `predict for table [live_data v] with classifier [gestureClassifier] show neighbors [yes v]` → read predicted class → trigger action based on gesture. They handle prediction confidence (some predictions are uncertain) and implement minimum confidence thresholds before accepting predictions. They test with gestures not in training data to see how classifier handles unknowns.

Dependencies:
* T10.G6.02: Sort a table by a column
* T23.G8.02.02: Train KNN classifier with collected gesture data
* T03.G6.01: Propose modules for a medium project
* T05.G6.01: Apply empathy, needs, and accessibility checklist to a design
* T07.G6.01: Trace nested loops with variable bounds


ID: T23.G8.03
Topic: T23 – AI Perception
Skill: Fuse voice, pose, and UI widgets into a cooperative simulation
Description: Students build a multi-user scenario (space mission, emergency response, surgical simulation) where different team members use different modalities simultaneously: one issues voice commands, another performs gestures to manipulate tools, a third confirms via widget buttons. The system coordinates timing, prevents conflicts (can't launch if gesture not confirmed), and displays live event log.

Dependencies:
* T08.G6.01: Use conditionals to control simulation steps
* T23.G7.02: Require multimodal confirmation (voice + gesture)
* T23.G7.03: Score a pose-based challenge with coaching tips
* T23.G6.03.01: Build a two-way voice chatbot loop
* T02.G6.01: Learn the pseudocode generation block
* T04.G6.01: Group snippets by underlying algorithm pattern
* T05.G6.01: Apply empathy, needs, and accessibility checklist to a design


ID: T23.G8.03.01
Topic: T23 – AI Perception
Skill: Evaluate classifier performance using confusion matrices
Description: Students systematically evaluate KNN classifier performance by creating confusion matrices. They test the classifier with labeled test data, record predicted vs actual classes in a matrix table, and calculate metrics: accuracy (correct predictions / total predictions), per-class precision (true positives / predicted positives), and per-class recall (true positives / actual positives). They identify which gesture pairs get confused most often (e.g., "peace sign" confused with "pointing") and use this analysis to improve training data or feature selection.

Dependencies:
* T10.G6.02: Sort a table by a column
* T23.G8.02.03: Deploy trained classifier to recognize live gestures
* T07.G6.01: Trace nested loops with variable bounds
* T09.G6.01: Model real-world quantities using variables and formulas
* T10.G6.01: Sort a table by a column


ID: T23.G8.04
Topic: T23 – AI Perception
Skill: Publish a privacy and deployment plan for perception apps
Description: Students research real voice/vision privacy concerns (storage duration, consent requirements, data retention policies, third-party access) and write a comprehensive policy for their app. They document: what data is captured, how long it's stored, who can access it, how to request deletion, when to use offline modes, and fallback behaviors. They reference their own logging/calibration/fairness features and align with T05 design thinking principles.

Dependencies:
* T04.G6.01: Group snippets by underlying algorithm pattern
* T08.G6.01: Use conditionals to control simulation steps
* T23.G7.05: Implement fairness safeguards for perception systems
* T23.G6.08: Add consent and privacy controls for sensor use
* T05.G6.01: Apply empathy, needs, and accessibility checklist to a design
* T07.G6.01: Trace nested loops with variable bounds
* T13.G6.01: Trace complex code with multiple variables


ID: T23.G8.04.01
Topic: T23 – AI Perception
Skill: Experiment with different K values in KNN classification
Description: Students systematically experiment with K parameter in KNN classification. They train classifiers with K=1, K=3, K=5, K=7, K=9 using the same training data and evaluate each on test data. They observe patterns: K=1 is sensitive to noise and outliers (overfitting), large K over-smooths decision boundaries (underfitting), odd K values avoid ties in voting. They plot accuracy vs K to find optimal value and understand that optimal K depends on dataset characteristics (size, noise level, class overlap).

Dependencies:
* T10.G6.02: Sort a table by a column
* T23.G8.03.01: Evaluate classifier performance using confusion matrices
* T03.G6.01: Propose modules for a medium project
* T07.G6.01: Trace nested loops with variable bounds
* T09.G6.01: Model real-world quantities using variables and formulas


ID: T23.G8.05
Topic: T23 – AI Perception
Skill: Evaluate societal impacts of perception AI systems
Description: Students analyze real-world examples of AI perception systems (facial recognition in law enforcement, voice assistants in homes, gesture controls in healthcare) and evaluate benefits and risks for different communities. They propose ethical guidelines for responsible deployment: when to use perception AI, when not to, required safeguards, transparency requirements, and community oversight mechanisms.

Dependencies:
* T08.G6.01: Use conditionals to control simulation steps
* T09.G6.01: Model real-world quantities using variables and formulas
* T23.G7.04: Monitor detection accuracy across different users
* T23.G7.05: Implement fairness safeguards for perception systems
* T02.G6.01: Learn the pseudocode generation block
* T05.G6.01: Apply empathy, needs, and accessibility checklist to a design
* T10.G6.01: Sort a table by a column


ID: T23.G8.05.01
Topic: T23 – AI Perception
Skill: Apply feature engineering to improve gesture recognition accuracy
Description: Students improve gesture classifier performance through feature engineering. They experiment with different feature sets: raw finger curl/dir values, derived features (finger spread = max curl - min curl, hand openness = average curl), normalized features (scale x/y to 0-1 range), and feature combinations. They compare classifier accuracy with different feature sets and understand that good features highlight differences between classes. They learn to identify and remove irrelevant or redundant features that add noise without improving accuracy.

Dependencies:
* T10.G6.02: Sort a table by a column
* T23.G8.03.01: Evaluate classifier performance using confusion matrices
* T02.G6.01: Learn the pseudocode generation block
* T07.G6.01: Trace nested loops with variable bounds
* T08.G6.01a: Use conditionals in physics simulations


ID: T23.G8.06
Topic: T23 – AI Perception
Skill: Introduction to neural networks and how they differ from KNN
Description: Students learn the fundamental differences between KNN and neural networks for classification. They understand that KNN stores training examples and compares new data to stored examples (instance-based learning), while neural networks learn patterns and create a model (parametric learning). They explore trade-offs: KNN is simple but slow for large datasets and requires storing all training data; neural networks are complex but fast at prediction time and can learn complex patterns. They compare when to use each approach.

Dependencies:
* T08.G6.01: Use conditionals to control simulation steps
* T23.G8.04.01: Experiment with different K values in KNN classification
* T05.G6.01: Apply empathy, needs, and accessibility checklist to a design
* T07.G6.01: Trace nested loops with variable bounds
* T10.G6.01: Sort a table by a column


ID: T23.G8.07
Topic: T23 – AI Perception
Skill: Practice using pre-trained neural network models
Description: Students use pre-trained neural network models in CreatiCode for perception tasks (pose estimation, speech recognition). They understand that pre-trained models have been trained on large datasets and can recognize common patterns without custom training. They load pre-trained models (the built-in detection blocks use neural networks), feed input data, interpret outputs, and compare performance to custom KNN classifiers. They learn when pre-trained models are appropriate (common tasks, limited training data) vs when custom training is needed (specialized gestures, domain-specific recognition).

Dependencies:
* T08.G6.01: Use conditionals to control simulation steps
* T23.G8.06: Introduction to neural networks and how they differ from KNN
* T05.G6.01: Apply empathy, needs, and accessibility checklist to a design
* T07.G6.01: Trace nested loops with variable bounds
* T10.G6.01: Sort a table by a column


ID: T23.G8.08
Topic: T23 – AI Perception
Skill: Build a custom neural network for gesture classification
Description: Students design and train a simple neural network for gesture classification using CreatiCode's neural network blocks: `create_nn_model`, `addlayertomodel`, `compile_model`, `train_model`, `predict_by_model`. They specify network architecture (input layer size = number of features, hidden layer size, output layer size = number of gesture classes), configure training parameters (learning rate, epochs), train the network with collected gesture data, and deploy for real-time recognition. They compare neural network performance to their KNN classifier and understand that neural networks can learn more complex patterns but require more training data.

Dependencies:
* T08.G6.01: Use conditionals to control simulation steps
* T23.G8.07: Practice using pre-trained neural network models
* T23.G8.02.02: Train KNN classifier with collected gesture data
* T03.G6.01: Propose modules for a medium project
* T07.G6.01: Trace nested loops with variable bounds
* T10.G6.01: Sort a table by a column


ID: T23.G8.09
Topic: T23 – AI Perception
Skill: Save and load trained neural network models
Description: Students learn to persist trained neural network models for reuse using `save_model` and `load_model` blocks. They train a model once and reuse it across sessions, share models with other users, create model libraries for different tasks, and version models (save model_v1, model_v2 as improvements are made). They understand the benefits: avoid retraining (save time), ensure consistency (same model across deployments), and enable offline usage (load model without requiring training data). They implement model versioning and testing workflows.

Dependencies:
* T08.G6.01: Use conditionals to control simulation steps
* T23.G8.08: Build a custom neural network for gesture classification
* T07.G6.01: Trace nested loops with variable bounds
* T10.G6.01: Sort a table by a column
* T12.G6.01: Analyze a program's structure using a checklist and suggest specific improvements


ID: T23.G8.10
Topic: T23 – AI Perception
Skill: Use semantic search to match voice commands to intents
Description: Students implement semantic search for flexible voice command recognition. Instead of exact phrase matching ("open map" only), they use semantic similarity to match variations ("show the map," "display map," "I need a map") to the same intent. They use NLP intent classification (from T23.G6.11) to handle paraphrasing, synonyms, and natural language variations. They build a voice command system that understands user intent rather than requiring exact phrasing.

Dependencies:
* T22.G7.01: Compare completion vs chat models and choose the appropriate one
* T23.G6.11: Use NLP sentence analysis to extract parts of speech
* T03.G6.01: Propose modules for a medium project
* T04.G6.01: Group snippets by underlying algorithm pattern
* T10.G6.01: Sort a table by a column


ID: T23.G8.11
Topic: T23 – AI Perception
Skill: Implement AI-powered content moderation in chat applications
Description: Students add content moderation to voice-based chat applications using AI moderation APIs. They implement filters that detect and block inappropriate content: profanity, hate speech, personal information, and unsafe topics. They handle moderation results: reject unsafe messages, provide user feedback ("message blocked: inappropriate content"), log moderation events, and implement escalation procedures for repeated violations. They understand the importance of moderation for safe user experiences and explore limitations (false positives, cultural context).

Dependencies:
* T22.G6.01: Trace how a chatbot script processes each turn
* T23.G6.03.01: Build a two-way voice chatbot loop
* T05.G6.01: Apply empathy, needs, and accessibility checklist to a design
* T06.G6.01: Trace event execution paths in a multi‑event program
* T08.G6.01a: Use conditionals in physics simulations


ID: T23.G8.12.01
Topic: T23 – AI Perception
Skill: Define ML problem and success metrics
Description: Students define a clear machine learning problem statement for their perception application: what should the system detect/classify, what constitutes success, and how will performance be measured. They specify success metrics: target accuracy (e.g., >90% gesture recognition), acceptable latency (e.g., <500ms response time), and fairness criteria (similar accuracy across user groups). They document assumptions, constraints, and requirements before beginning data collection or model development.

Dependencies:
* T08.G6.01: Use conditionals to control simulation steps
* T23.G8.09: Save and load trained neural network models
* T23.G8.03.01: Evaluate classifier performance using confusion matrices
* T07.G6.01: Trace nested loops with variable bounds
* T09.G6.01: Model real-world quantities using variables and formulas
* T10.G6.01: Sort a table by a column


ID: T23.G8.12.02
Topic: T23 – AI Perception
Skill: Plan data collection strategy with quality checks
Description: Students design a comprehensive data collection strategy: determine sample size per class (minimum 50 samples), ensure diversity (different users, lighting conditions, backgrounds), implement quality checks (reject blurry images, incomplete data), and document collection procedures. They create data collection protocols that other team members can follow, ensuring consistent and high-quality training data. They understand that data quality directly impacts model performance.

Dependencies:
* T08.G6.01: Use conditionals to control simulation steps
* T23.G8.12.01: Define ML problem and success metrics
* T23.G8.02.01: Create data collection UI for gesture samples
* T07.G6.01: Trace nested loops with variable bounds
* T09.G6.01: Model real-world quantities using variables and formulas
* T10.G6.01: Sort a table by a column


ID: T23.G8.12.03
Topic: T23 – AI Perception
Skill: Document ML workflow and deployment plan
Description: Students create comprehensive documentation for their complete ML workflow covering all stages: (1) problem definition and success metrics, (2) data collection strategy and quality assurance, (3) exploratory data analysis and feature engineering, (4) model selection and training, (5) evaluation and iteration, (6) deployment and monitoring, (7) maintenance and updates. They document testing procedures, performance benchmarks, deployment considerations (resource requirements, fallback behaviors), and maintenance plans (when to retrain, how to handle drift). This capstone skill demonstrates the full ML lifecycle.

Dependencies:
* T08.G6.01: Use conditionals to control simulation steps
* T23.G8.12.02: Plan data collection strategy with quality checks
* T23.G8.04: Publish a privacy and deployment plan for perception apps
* T07.G6.01: Trace nested loops with variable bounds
* T09.G6.01: Model real-world quantities using variables and formulas
* T10.G6.01: Sort a table by a column
